[
["index.html", "STAT 245 Course Notes Chapter 1 Description", " STAT 245 Course Notes Stacy DeRuiter, Calvin University 2019-10-17 Chapter 1 Description This is a set of course notes distributed in STAT 245 at Calvin University in Fall 2019. Contact sld33 at calvin.edu with comments, corrections or suggestions. "],
["linear-regression.html", "Chapter 2 Linear Regression 2.1 Data 2.2 Simple linear regression, Residuals &amp; Least squares 2.3 Multiple regression 2.4 Predictors with two categories 2.5 Returning to the R Model Summary 2.6 Predictions from the model 2.7 Why are we doing this again? 2.8 Shortcut Method - With Uncertainty 2.9 DIY Method", " Chapter 2 Linear Regression You probably learned something about linear regression in a previous course. Here, we briefly review the main concepts of simple linear regression and quickly expand our tool box to multiple regression (with both quantitative and categorical predictors). 2.1 Data We will consider a small dataset from an article by J.S. Martin and colleagues, titled Facial width-to-height ratio is associated with agonistic and affiliative dominance in bonobos (Pan paniscus) Notes: variable fWHR is the facial width-height ratio and AssR is the Assertiveness score of affiliative dominance. normDS is another dominance score. A few figures of the data are below - we will do some more exploration together. ## Observations: 117 ## Variables: 8 ## $ Name &lt;fct&gt; Zuani, Zuani, Zorba, Zorba, Zorba, Zomi, Zomi, Zamba, Z... ## $ Group &lt;fct&gt; Apenheul, Apenheul, Wilhelma, Wilhelma, Wilhelma, Frank... ## $ Sex &lt;fct&gt; Female, Female, Male, Male, Male, Female, Female, Male,... ## $ Age &lt;int&gt; 22, 22, 34, 34, 34, 15, 15, 14, 14, 14, 18, 18, 18, 18,... ## $ fWHR &lt;dbl&gt; 1.475052, 1.321814, 1.581446, 1.479237, 1.390086, 1.340... ## $ AssR &lt;dbl&gt; 5.36, 5.36, 2.36, 2.36, 2.36, 3.92, 3.92, 4.74, 4.74, 4... ## $ normDS &lt;dbl&gt; 1.430, 1.430, 2.341, 2.341, 2.341, 3.087, 3.087, 3.035,... ## $ weight &lt;dbl&gt; 24.0, 24.0, NA, NA, NA, NA, NA, 41.6, 41.6, 41.6, 38.0,... 2.2 Simple linear regression, Residuals &amp; Least squares First, let’s review and consider a simple (one-predictor) linear regression model. Fit the model slr &lt;- lm(fWHR ~ AssR, data=bonobos) Extract the slope and intercept values: coef(slr) ## (Intercept) AssR ## 1.30685287 0.02918242 Add the regression line to the plot: gf_point(fWHR ~ AssR, data=bonobos) %&gt;% gf_lm() summary(slr) ## ## Call: ## lm(formula = fWHR ~ AssR, data = bonobos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31320 -0.11369 -0.01242 0.09008 0.49241 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.30685 0.06283 20.801 &lt;2e-16 *** ## AssR 0.02918 0.01420 2.055 0.0421 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1689 on 115 degrees of freedom ## Multiple R-squared: 0.03542, Adjusted R-squared: 0.02704 ## F-statistic: 4.223 on 1 and 115 DF, p-value: 0.04213 2.2.1 Using lm() to fit a linear regression in R 2.2.2 Equation of the fitted regression line 2.3 Multiple regression Rarely does our response variable really depend on only one predictor. Can we improve the model by adding more predictors? mlr &lt;- lm(fWHR ~ AssR + weight, data=bonobos) coef(mlr) ## (Intercept) AssR weight ## 0.944790930 0.039888045 0.008644299 2.3.1 Is it really better? How do we know if the model with more predictors is “better”? (For a more detailed answer, wait about a week…) But before we can define a “beter” model: how did R find the “best” intercept and slopes? 2.3.2 Regression residuals = “errors” 2.3.3 Computing Predictions Use the regression equation to compute predicted values for the three data points below: ## fWHR AssR weight ## 8 1.880866 4.74 41.6 ## 25 1.798387 5.38 50.6 ## 41 1.591440 3.97 NA ## 65 1.545019 4.87 38.5 2.4 Predictors with two categories mlr2 &lt;- lm(fWHR ~ AssR + weight + Sex, data = bonobos) coef(mlr2) ## (Intercept) AssR weight SexMale ## 1.065420976 0.058435841 0.002257142 0.128484275 How does the model incorporate this covariate mathematically? 2.4.1 Predictors with more categories gf_boxplot(fWHR ~ Group, data = bonobos) mlr3 &lt;- lm(fWHR ~ AssR + weight + Sex + Group, data = bonobos) coef(mlr3) ## (Intercept) AssR weight SexMale ## 1.007734691 0.064361973 0.003458979 0.124854271 ## GroupFrankfurt GroupPlanckendael GroupTwycross GroupWilhelma ## 0.037426358 -0.008464572 -0.112907589 0.011186724 ## GroupWuppertal ## -0.004364826 How does the model incorporate this covariate mathematically? 2.5 Returning to the R Model Summary There are several bits of information you should be able to extract from the summary() output R produces on a fitted linear regression model: \\(\\beta\\)s, Coefficient Estimates \\(\\sigma\\), labeled “residual standard error” \\(R^2\\) (adjusted) mlr3 &lt;- lm(fWHR ~ AssR + weight + Sex + Group, data = bonobos) summary(mlr3) ## ## Call: ## lm(formula = fWHR ~ AssR + weight + Sex + Group, data = bonobos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.38288 -0.09488 -0.02642 0.07196 0.48464 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.007735 0.217585 4.631 2.05e-05 *** ## AssR 0.064362 0.021158 3.042 0.0035 ** ## weight 0.003459 0.005547 0.624 0.5353 ## SexMale 0.124854 0.059278 2.106 0.0394 * ## GroupFrankfurt 0.037426 0.074892 0.500 0.6191 ## GroupPlanckendael -0.008465 0.075407 -0.112 0.9110 ## GroupTwycross -0.112908 0.074779 -1.510 0.1364 ## GroupWilhelma 0.011187 0.085538 0.131 0.8964 ## GroupWuppertal -0.004365 0.071292 -0.061 0.9514 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1691 on 59 degrees of freedom ## (49 observations deleted due to missingness) ## Multiple R-squared: 0.2517, Adjusted R-squared: 0.1502 ## F-statistic: 2.48 on 8 and 59 DF, p-value: 0.02167 2.6 Predictions from the model 2.6.1 By Hand The equation for the fitted model above is: \\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3I_{Male} + \\beta_4I_{Frankfurt} + \\beta_5I_{Planckendael} + \\beta_6I_{Twycross} + \\beta_7I_{Wilhelma} + \\beta_7I_{Wuppertal} + \\epsilon\\] where \\(y =\\) \\(\\beta_0=\\) \\(x_1=\\) \\(x_2=\\) \\(\\beta_1, \\beta_2, \\beta_3 ...\\) are: \\(I_{Male} =\\) \\(I_{Frankfurt} =\\) \\(I_{Planckendael} =\\) , etc. \\(\\epsilon=\\) 2.6.1.1 Comprehension check: What is the expected fWHR (according to this model) for a 30 kg female bonobo at the Wilhelma zoo? 2.6.2 Prediction Plots in R We can ask R to compute predictions for all the data points in the real dataset. bonobos &lt;- bonobos %&gt;% mutate(preds = predict(mlr3)) ## Error: Column `preds` must be length 117 (the number of rows) or one, not 68 Wait, what? This error is because the lm() function removes rows containing missing values from the dataset, so it computes only 68 residuals (for the complete cases in the data). This doesn’t match the 117 rows in the original data. We can solve the problem by omitting rows with missing values first. To be safe, we first select only the variables we need, so we don’t omit rows based on missing values in unused variables. b2 &lt;- bonobos %&gt;% dplyr::select(fWHR, weight, AssR, Sex, Group) %&gt;% na.omit() %&gt;% mutate(preds = predict(mlr3)) We have a full set of predictions! But if we plot these predictions on a scatter plot of fWHR as a function of AssR, we do not get a straight line, because the predictions are also impacted by varying values of weight, Sex, and Group: gf_point(fWHR ~ AssR, data = b2) %&gt;% gf_line(preds ~ AssR, data=b2) But…we would really like a straight line that helps us visualize the meaning of the \\(\\beta\\) (slope coefficient) for AssR. We can make predictions for a hypothetical dataset, in which AssR varies over a reasonable range, but the other predictors stay constant. This lets us see how AssR (and only AssR) affects the response, without contributions from other predictors. In choosing the values to include in hypothetical dataset, we often choose to hold variables constant at their most common or median values, but not blindly: also, avoid impossible or implausible variable combinations (for example, specifying that a person lives in the state of Michigan but the city of Chicago, or that they are a 5-year-old person with 4 children). In this case, to match the figures in the published paper, we are also going to vary the Sex - but generally you’d only allow one predictor to vary. fake_data &lt;- expand.grid(AssR = seq(from=1.8, to=5.7, by=0.05), weight = 38.5, Sex = c(&#39;Female&#39;, &#39;Male&#39;), Group = &#39;Wuppertal&#39;) fake_data &lt;- fake_data %&gt;% mutate(preds = predict(mlr3, newdata = fake_data)) gf_line(preds ~ AssR, color = ~Sex, data=fake_data) %&gt;% gf_labs(y=&#39;Predicted\\nfWHR&#39;) 2.6.2.1 Comprehension checks: Should we overlay prediction-plot line(s) on the data scatter plot? How do you think the plot would look if we changed the constant predictor values? What is missing from this picture? 2.6.2.2 Shortcut require(s245) pred_plot(mlr3, &#39;AssR&#39;) 2.7 Why are we doing this again? Why make prediction plots? 2.8 Shortcut Method - With Uncertainty We saw before that pred_plot() makes it very easy for us to generate prediction plots showing what a (multiple regression) model says about the relationship between the response and one of the predictors: require(s245) pred_plot(mlr3, &#39;AssR&#39;) %&gt;% gf_labs(y = &#39;Predicted fWHR&#39;) Note the custom axis label - otherwise you get a long, unwieldy default “Predictions from fitted model” require(s245) pred_plot(mlr3, &#39;Group&#39;) %&gt;% gf_labs(y = &#39;Predicted fWHR&#39;) They look nice! But they should raise two questions: Uncertainty: Fixed values: get_fixed(bonobos) %&gt;% pander::pander() Name Group Sex Age fWHR AssR normDS weight three pt_size Eja Twycross Female 21 1.412 4.51 2.368 40 no 1 2.8.1 Anatomy of a Confidence Interval pred_plot(mlr3, &#39;Sex&#39;) %&gt;% gf_labs(y = &#39;Predicted fWHR&#39;) 2.9 DIY Method 2.9.1 Creating a hypothetical dataset We would like to create a hypothetical dataset where one predictor variable varies, and all the rest stay fixed. Let’s choose AssR. We use expand.grid(): fake_data &lt;- expand.grid(AssR = seq(from=1.8, to=5.7, by=0.05), weight = 40, Sex = &#39;Female&#39;, Group = &#39;Twycross&#39;) glimpse(fake_data) ## Observations: 79 ## Variables: 4 ## $ AssR &lt;dbl&gt; 1.80, 1.85, 1.90, 1.95, 2.00, 2.05, 2.10, 2.15, 2.20, 2... ## $ weight &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,... ## $ Sex &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female,... ## $ Group &lt;fct&gt; Twycross, Twycross, Twycross, Twycross, Twycross, Twycr... Now, make predictions for our fake data. preds &lt;- predict(mlr3, newdata = fake_data, se.fit = TRUE) fake_data &lt;- fake_data %&gt;% mutate(fitted = preds$fit, se.fit = preds$se.fit) glimpse(fake_data) ## Observations: 79 ## Variables: 6 ## $ AssR &lt;dbl&gt; 1.80, 1.85, 1.90, 1.95, 2.00, 2.05, 2.10, 2.15, 2.20, 2... ## $ weight &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,... ## $ Sex &lt;fct&gt; Female, Female, Female, Female, Female, Female, Female,... ## $ Group &lt;fct&gt; Twycross, Twycross, Twycross, Twycross, Twycross, Twycr... ## $ fitted &lt;dbl&gt; 1.149038, 1.152256, 1.155474, 1.158692, 1.161910, 1.165... ## $ se.fit &lt;dbl&gt; 0.08347207, 0.08267088, 0.08187552, 0.08108616, 0.08030... How do we go from standard errors to confidence intervals? We can either do this before plotting, or while plotting. To do it before and add the results to the hypothetical dataset: fake_data &lt;- fake_data %&gt;% mutate(CI_lower = fitted - 1.96*se.fit, CI_upper = fitted + 1.96*se.fit) glimpse(fake_data) ## Observations: 79 ## Variables: 8 ## $ AssR &lt;dbl&gt; 1.80, 1.85, 1.90, 1.95, 2.00, 2.05, 2.10, 2.15, 2.20,... ## $ weight &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 4... ## $ Sex &lt;fct&gt; Female, Female, Female, Female, Female, Female, Femal... ## $ Group &lt;fct&gt; Twycross, Twycross, Twycross, Twycross, Twycross, Twy... ## $ fitted &lt;dbl&gt; 1.149038, 1.152256, 1.155474, 1.158692, 1.161910, 1.1... ## $ se.fit &lt;dbl&gt; 0.08347207, 0.08267088, 0.08187552, 0.08108616, 0.080... ## $ CI_lower &lt;dbl&gt; 0.9854326, 0.9902210, 0.9949980, 0.9997632, 1.0045164... ## $ CI_upper &lt;dbl&gt; 1.312643, 1.314291, 1.315950, 1.317621, 1.319304, 1.3... 2.9.2 Making the plot Now, we just need to plot! gf_line(fitted ~ AssR, data=fake_data) %&gt;% gf_labs(y=&#39;Predicted\\nfWHR&#39;) %&gt;% gf_ribbon(CI_lower + CI_upper ~ AssR, data = fake_data) If we wanted to figure out the CI bounds while plotting, we could calculate them on the fly like this: gf_line(fitted ~ AssR, data=fake_data) %&gt;% gf_labs(y=&#39;Predicted\\nfWHR&#39;) %&gt;% gf_ribbon((fitted - 1.96*se.fit ) + (fitted + 1.96*se.fit) ~ AssR, data = fake_data) (which will look just the same). 2.9.3 Categorical predictors What will be different if the predictor of interest is categorical? hypothetical data: plot: fake_sex_data &lt;- expand.grid(AssR = 4.51, weight = 40, Sex = c(&#39;Male&#39;, &#39;Female&#39;), Group = &#39;Twycross&#39;) preds &lt;- predict(mlr3, newdata = fake_sex_data, se.fit = TRUE) fake_sex_data &lt;- fake_sex_data %&gt;% mutate(fitted = preds$fit, se.fit = preds$se.fit) gf_point(fitted ~ Sex, data=fake_sex_data) %&gt;% gf_labs(y=&#39;Predicted fWHR&#39;) %&gt;% gf_errorbar((fitted - 1.96*se.fit ) + (fitted + 1.96*se.fit) ~ Sex, data = fake_sex_data) "],
["model-selection-using-information-criteria.html", "Chapter 3 Model Selection Using Information Criteria 3.1 Data and Model 3.2 Calculations 3.3 Decisions with ICs 3.4 All-possible-subsets Selection 3.5 Which IC should I use? 3.6 Quantities derived from AIC 3.7 Important Caution", " Chapter 3 Model Selection Using Information Criteria So far, we have learned to fit models with multiple predictors, both quantitative and categorical, and to assess whether required conditions are met for linear regression to be an appropriate model for a dataset. One missing piece is: If I have an appropriate model with a set of multiple predictors, how can I choose which predictors are worth retaining in a “best” model for the data (and which ones have no relationship, or a weak relationship, with the response, so should be discarded)? 3.1 Data and Model Today we will recreate part of the analysis from Vertebrate community composition and diversity declines along a defaunation gradient radiating from rural villages in Gabon, by Sally Koerner and colleagues. They investigated the relationship between rural villages, hunting, and wildlife in Gabon. They asked how monkey abundance depends on distance from villages, village size, and vegetation characteristics. They shared their data at Dryad.org and we can read it in and fit a regression model like this: defaun &lt;- read.csv(&#39;http://sldr.netlify.com/data/koerner_gabon_defaunation.csv&#39;) ape_mod &lt;- lm(RA_Apes ~ Veg_DBH + Veg_Canopy + Veg_Understory + Veg_Rich + Veg_Stems + Veg_liana + LandUse + Distance + NumHouseholds, data = defaun) summary(ape_mod) ## ## Call: ## lm(formula = RA_Apes ~ Veg_DBH + Veg_Canopy + Veg_Understory + ## Veg_Rich + Veg_Stems + Veg_liana + LandUse + Distance + NumHouseholds, ## data = defaun) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9857 -0.9419 -0.0360 0.8239 6.3832 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.752517 13.372210 0.430 0.6741 ## Veg_DBH -0.093171 0.073114 -1.274 0.2249 ## Veg_Canopy 0.670094 2.062545 0.325 0.7504 ## Veg_Understory -1.691235 2.071299 -0.817 0.4289 ## Veg_Rich 0.361960 0.480362 0.754 0.4646 ## Veg_Stems -0.097211 0.169073 -0.575 0.5751 ## Veg_liana -0.158505 0.253031 -0.626 0.5419 ## LandUseNeither 1.696755 2.058937 0.824 0.4247 ## LandUsePark -2.947189 2.222710 -1.326 0.2077 ## Distance 0.302626 0.119865 2.525 0.0254 * ## NumHouseholds -0.002107 0.043458 -0.048 0.9621 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.725 on 13 degrees of freedom ## Multiple R-squared: 0.5439, Adjusted R-squared: 0.1931 ## F-statistic: 1.551 on 10 and 13 DF, p-value: 0.2262 as.numeric(logLik(ape_mod)) ## [1] -50.75799 3.2 Calculations Information criteria allow us to balance the conflicting goals of having a model that fits the data as well as possible (which pushes us toward models with more predictors) and parsimony (choosing the simplest model, with the fewest predictors, that works for the data and research question). The basic idea is that we minimize the quantity \\(-(2LogLikelihood - penalty) = -2LogLikelihood + penalty\\) AIC is computed according to \\(-2LogLikelihood +2k\\), where \\(k\\) is the number of coefficients being estimated (don’t forget \\(\\sigma\\)!) Smaller AIC is better. BIC is computed according to \\(-2LogLikelihood + ln(n)k\\), where \\(n\\) is the number of observations (rows) in the dataset and \\(k\\) is the number of coefficients being estimated. Smaller BIC is better. Verify that the BIC for this model is 139.65. 3.3 Decisions with ICs The following rules of thumb (not laws, just common rules of thumb) may help you make decisions with ICs: A model with lower IC by at least 3 units is notably better. If two or more models have ICs within 3 IC units of each other, there is not a lot of difference between them. Here, we usually choose the model with fewest predictors. In some cases, if the research question is to measure the influence of some particular predictor on the response, but the IC does not strongly support including that predictor in the best model (IC difference less than 3), you might want to keep it in anyway and then discuss the situation honestly, for example, “AIC does not provide strong support for including predictor x in the best model, but the model including predictor x indicates that as x increases the response decreases slightly. More research would be needed…” 3.4 All-possible-subsets Selection The model we just fitted is our full model, with all predictors of potential interest included. How can we use information criteria to choose the best model from possible models with subsets of the predictors? We can use the dredge() function from the MuMIn package to get and display ICs for all these models. Before using dredge, we need to make sure our dataset has no missing values, and also set the “na.action” input for our model (can be done in call to lm(..., na.action = 'na.fail') also). require(MuMIn) ape_mod &lt;- ape_mod %&gt;% update(na.action = &#39;na.fail&#39;) ape_dredge &lt;- dredge(ape_mod, rank=&#39;BIC&#39;) pander::pander(head(ape_dredge, 7)) Table continues below (Intercept) Distance LandUse NumHouseholds Veg_Canopy 258 8.753 0.195 NA NA NA 2 -0.6912 0.2303 NA NA NA 274 11.44 0.1848 NA NA NA 322 11.9 0.2033 NA NA NA 290 9.805 0.1884 NA NA NA 386 9.49 0.1976 NA NA NA 266 7.783 0.1896 NA NA 0.2771 Table continues below Veg_DBH Veg_liana Veg_Rich Veg_Stems Veg_Understory df 258 NA NA NA NA -2.988 4 2 NA NA NA NA NA 3 274 -0.04551 NA NA NA -3.144 5 322 NA NA -0.1939 NA -3.11 5 290 NA -0.09802 NA NA -2.952 5 386 NA NA NA -0.03113 -2.904 5 266 NA NA NA NA -2.964 5 logLik BIC delta weight 258 -53.9 120.5 0 0.3284 2 -55.8 121.1 0.6241 0.2404 274 -53.38 122.7 2.146 0.1123 322 -53.55 123 2.491 0.09449 290 -53.67 123.2 2.727 0.08399 386 -53.82 123.5 3.03 0.0722 266 -53.88 123.7 3.144 0.0682 What is the best model according to BIC, for this dataset? 3.5 Which IC should I use? AIC and BIC may give different best models, especially if the dataset is large. You may want to just choose one to use a priori (before making calculations). You might prefer BIC if you want to err on the “conservative” side, as it is more likely to select a “smaller” model with fewer predictors. This is because of its larger penalty. 3.6 Quantities derived from AIC \\(\\Delta AIC\\) is the AIC for a given model, minus the AIC of the best one in the dataset. (Same for \\(\\Delta BIC\\)) Akaike weights are values (ranging from 0-1) that measure the weight of evidence suggesting that a model is the best one (given that there is one best one in the set) 3.7 Important Caution Very important: IC can ONLY be compared for models with the same response variable, and the exact same rows of data. "],
["likelihood.html", "Chapter 4 Likelihood 4.1 Data 4.2 Review - the Normal probability density function (PDF) 4.3 A simple model 4.4 Using the Model to Make Predictions 4.5 Likelihood to the Rescue! 4.6 How does this relate to linear regression? 4.7 Likelihood of a dataset, given a model", " Chapter 4 Likelihood In the last section, we said that “likelihood” is a measure of goodness-of-fit of a model to a dataset. But what is it exactly and just how do we compute it? 4.1 Data Today’s dataset was collected in Senegal in 2015-2016 in a survey carried out by UNICEF, of 5440 households in the urban area of Dakar, Senegal. Among these households, information was collected about 4453 children under 5 years old, including their gf_dhistogram(~AN3, data=wt, binwidth=1) %&gt;% gf_labs(x=&#39;Weight (kg)&#39;, y=&#39;Probability\\nDensity&#39;) %&gt;% gf_fitdistr(dist=&#39;dnorm&#39;, size=1.3) %&gt;% gf_refine(scale_x_continuous(breaks=seq(from=0, to=30, by=2))) 4.2 Review - the Normal probability density function (PDF) \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] 4.3 A simple model The distribution of weights looks quite unimodal and symmetric, so we will model it with a normal distribution with mean 11.8 and standard deviation 3.53 (N( \\(\\mu=\\) 11.8, \\(\\sigma=\\) 3.53), black line). 4.4 Using the Model to Make Predictions If you had to predict the weight of one child from this population, what weight would you guess? Is it more likely for a child in Dakar to weigh 10kg, or 20kg? How much more likely? What is the probability of a child in Dakar weighing 11.5 kg? 4.5 Likelihood to the Rescue! Which is more likely: three children who weigh 11, 8.2, and 13kg, or three who weigh 10, 12.5 and 15 kg? How did you: Find the likelihood of each observation? Combine the likelihoods of a set of three observations? What did you have to assume about the set of observations? 4.6 How does this relate to linear regression? What if we think of this situation as a linear regression problem (with no predictors)? lm_version &lt;- lm(AN3 ~ 1, data = wt) summary(lm_version) ## ## Call: ## lm(formula = AN3 ~ 1, data = wt) ## ## Residuals: ## &lt;Labelled double&gt;: Poids de l&#39;enfant (kilogrammes) ## Min 1Q Median 3Q Max ## -9.8964 -2.3964 0.1036 2.4036 18.9036 ## ## Labels: ## value label ## 99.9 poids non mesuré ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.79644 0.05435 217.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.529 on 4216 degrees of freedom ## (235 observations deleted due to missingness) 4.6.1 Model Equation: 4.7 Likelihood of a dataset, given a model Finally, now, we can understand what we were computing when we did logLik(lm_version) ## &#39;log Lik.&#39; -11301.19 (df=2) For our chosen regression model, we know that the residuals should have a normal distribution with mean 0 and standard deviation \\(\\sigma\\) (estimated Residual Standard Error from R summary() output). For each data point in the dataset, for a given regression model, we can compute a model prediction. We can subtract the prediction from the observed response-variable values to get the residuals. We can compute the likelihood (\\(L\\)) of this set of residuals by finding the likelihood of each individual residual \\(e_i\\) in a \\(N(0, \\sigma)\\) distribution. To get the likelihood of the full dataset given the model, we use the fact that the residuals are independent (they better be, because that was one of the conditions of of linear regression model) – we can multiply the likelihoods of all the individual residuals together to get the joint likelihood of the full set. That is the “likelihood” that is used in the AIC and BIC calculations we considered earlier. "],
["pdfs-and-pmfs.html", "Chapter 5 PDFs and PMFs 5.1 Beyond Normal 5.2 Types of probability distributions 5.3 Relevant Features of Distributions 5.4 Examples of Continuous Distributions 5.5 Examples of Discrete Distributions", " Chapter 5 PDFs and PMFs 5.1 Beyond Normal In our exploration of likelihoods, we did a little bit of work with the Normal probability density function. Here, we will state some characteristics of the normal distribution slightly more formally, and then we will get familiar with some other probability distributions (a.k.a. “the normal distribution’s wierd friends”). 5.2 Types of probability distributions 5.2.1 Continuous distributions The probability distribution of a continuous variable (one that can take on continuous real values, at least within a certain range) is called a probability density function or PDF for short. PDFs are functions of one continuous variable (we’ll call it \\(x\\)) that have two properties in common: The total area under the curve is 1 (\\(\\int_{-\\infty}^{\\infty} f(x)dx = 1\\)) The values of the function are non-negative (0, or a positive real number) for all real \\(x\\) (\\(f(x) \\geq 0 \\forall x \\in \\mathbb{R}\\)) For PDFs, the function values (\\(y\\)-axis values) are probability densities, useful for computing likelihoods; probabilities are given by finding areas under the curve. 5.2.2 Discrete Distributions We use categorical as well as quantitative variables in our regression models, so it will prove useful to have some discrete probability distributions as well as continuous ones. Discrete distributions associate each possible value of a discrete variable with its probability of occurence. A discrete probability distribution is characterized by a probability mass function or PMF for short (this is the discrete equivalent of a PDF). A PMF is a function of one discrete variable (we’ll call it \\(x\\)) that have two properties in common: The sum of all the function’s values is 1 (\\(\\sum_{x \\in S} f(x) =1\\), where \\(S\\) is the set of all possible values \\(x\\) can have) All values of the function are between 0 and 1 inclusive (they are probabilities) (\\(f(x) \\in [0,1] \\forall x \\in S\\)) 5.3 Relevant Features of Distributions For this course, the most important features to note about each probability distribuiton will be: The type of distribution: discrete or continuous? The support of the distribution: what range of possible values can the random variable \\(X\\) take on? The parameters of the distribution. Changing the parameters tunes the center and shape of the distribution, so these are what we need to estimate to fit a particular kind of distribution to a specific dataset. (For example, the parameters of the normal distribution are the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\).) The shape of the distribution: what shapes can the function take one? (For example, the normal distribution is always unimodal and symmetric.) The PDF or PMF of the distribution. What mathematical expression controls the shape of the distribution? We will also note a few examples of variables that might be well modelled by each distribution. 5.4 Examples of Continuous Distributions 5.4.1 Normal 5.4.1.0.1 Type Continuous 5.4.1.0.2 Support All real numbers 5.4.1.0.3 Parameters \\(\\mu\\), the mean, which can take on any real value \\(\\sigma\\), the standard deviation, which can take on any positive real value 5.4.1.0.4 Shapes The shape is always unimodal and symmetric. 5.4.1.0.5 PDF or PMF The normal distribution has PDF: \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] 5.4.1.0.6 Examples A normal distribution might be a good fit for data on childrens’ weights in kg, or for the duration of visits at a zoo, or… 5.4.2 Gamma 5.4.2.0.1 Type Continuous 5.4.2.0.2 Support positive real numbers 5.4.2.0.3 Parameters There are two alternate but equivalent parameterizations for the gamma distribution. The first option: \\(\\alpha\\) (shape) and \\(\\beta\\) (rate), where \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\) The second option: \\(k\\) (shape) and \\(\\theta\\) (scale), where \\(k &gt;0\\) and \\(\\theta&gt;0\\). Converting between the two parameterizations: \\(\\alpha = k\\) and \\(\\beta = \\frac{1}{\\theta}\\). 5.4.2.0.4 Shapes The gamma distribution can take on a unimodal, symmetric shape or a unimodal shape with any amount of right skew (up to an exponential distribution shape). Note: you don’t need to be familiar with exactly how the different values of parameters influence the shape of the gamma distribution PDF, so the curves here are not labelled with parameter values. 5.4.2.0.5 PDF or PMF The gamma distribution has PDF: \\[ f(x) = \\frac{1}{\\Gamma(k) \\theta^k}x^{(k-1)}e^{\\frac{-x}{\\theta}}\\] Where \\(\\Gamma\\) is the Gamma function (look up the definition if you choose), or equivalently, \\[ f(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{(\\alpha-1)}e^{-\\beta x}\\] 5.4.2.0.6 Examples Gamma distributions are often used to model things like wind speed or duration of an event (any quantity that might have right skew and is never negative). 5.4.3 Beta 5.4.3.0.1 Type Continuous 5.4.3.0.2 Support Real numbers between 0 and 1 ([0,1]) 5.4.3.0.3 Parameters \\(\\alpha\\) (shape 1) and \\(\\beta\\) (shape 2), both of which must be \\(&gt;1\\). 5.4.3.0.4 Shapes This distribution can take on almost any shape, for example: gf_dist(&#39;beta&#39;, params = c(shape1=5, shape2=1)) %&gt;% gf_dist(&#39;beta&#39;, params = c(shape1=1, shape2=3), color=colrs[1]) %&gt;% gf_dist(&#39;beta&#39;, params = c(shape1=2, shape2=2), color=colrs[2]) %&gt;% gf_dist(&#39;beta&#39;, params = c(shape1=2, shape2=5), color=colrs[3]) %&gt;% gf_dist(&#39;beta&#39;, params = c(shape1=0.5, shape2=0.5), color=colrs[4]) %&gt;% gf_labs(x=&#39;Possible Values of Variable&#39;, y=&#39;Probability Density&#39;) %&gt;% gf_lims(y=c(0,2.5)) 5.4.3.0.5 PDF or PMF The PDF is: \\[ f(x) = \\frac{x^{(\\alpha -1)}(1-x)^{(\\beta-1)}}{ B(\\alpha, \\beta)}\\] Where \\(B\\) is the Beta function (again, feel free to look up the definition if you are interested). 5.4.3.0.6 Examples Beta distributions could be used for any variable that takes on values between 0-1, for example, baseball players’ batting averages, or test scores (as proportions). 5.5 Examples of Discrete Distributions 5.5.1 Binomial 5.5.1.0.1 Type Discrete 5.5.1.0.2 Support You can think about the support of this distribution two ways. Technically, the support is \\(k= 0, 1,2,3,\\dots\\): 0 and positive integers, interpreted as the number of “successes” in \\(n\\) binomial trials. Binomial trials are independent observations of a process that has two possible outcomes, “success” or “failure”, with set probabilities of each occurring. (And probabilities of success and failure must sum to 1.) So, for each individual binomial trial, the possible outcomes are 0 and 1, often interpreted as TRUE and FALSE or “success” and “failure”. For our purposes, this distribution will be useful for modelling response variables that are categorical with two possible values (and the \\(n\\) trials will be the \\(n\\) rows in our dataset). 5.5.1.0.3 Parameters Parameters are \\(n\\), the number of independent trials, and \\(p\\), the probability of success in each trial. 5.5.1.0.4 Shapes The figure below shows the shape of binomial distributions with fixed \\(n=100\\) and varying \\(p\\): gf_dist(&#39;binom&#39;, params = c(size=100, prob=0.1), color=colrs[1]) %&gt;% gf_dist(&#39;binom&#39;, params = c(size=100, prob=0.25), color=colrs[2]) %&gt;% gf_dist(&#39;binom&#39;, params = c(size=100, prob=0.5), color=colrs[3]) %&gt;% gf_dist(&#39;binom&#39;, params = c(size=100, prob=0.75), color=colrs[4]) %&gt;% gf_dist(&#39;binom&#39;, params = c(size=100, prob=0.99), color=colrs[5]) %&gt;% gf_labs(x=&#39;k, the Number of Successes in 100 Trials&#39;, y=&#39;Probability&#39;) The \\(p\\) used were 0.1, 0.25, 0.5, 0.75, and 0.99. Can you tell which is which? 5.5.1.0.5 PDF or PMF The PMF for the binomial distribution is: \\[ P(X=k \\vert n,p) = {n \\choose k} p^k (1-p)^{n-k}\\] Where \\(k\\) is the number of successes observed in \\(n\\) trials (you can think of \\(k\\) as our “x-axis variable” for this PMF). 5.5.1.0.6 Examples We might use this distribution to model any categorical variable with two possible values, like Age (if possible values are “adult” and “child”) or health status (“has disease” or “does not have disease”). We’ll think of each observation in the dataset as one of the \\(n\\) indpendent trials, with one of two possible outcomes for each trial. 5.5.2 Poisson 5.5.2.0.1 Type Discrete 5.5.2.0.2 Support The support is 0 and positive integers (i.e., this distribution works well for count data). 5.5.2.0.3 Parameters The Poisson distribution has one parameter, \\(\\lambda\\) (the event rate per unit time) which must be greater than 0. 5.5.2.0.4 Shapes The distribution can take on unimodal shapes with varying amounts of right skew (from none, to an exponential shape). colrs &lt;- RColorBrewer::brewer.pal(8, &#39;Set2&#39;) gf_dist(&#39;pois&#39;, params=c(lambda=0.5), color=colrs[1]) %&gt;% gf_dist(&#39;pois&#39;, params=c(lambda=3), color=colrs[2]) %&gt;% gf_dist(&#39;pois&#39;, params=c(lambda=6), color=colrs[3]) %&gt;% gf_dist(&#39;pois&#39;, params=c(lambda=15), color=colrs[4]) %&gt;% gf_labs(x=&#39;k (number of events)&#39;, y=&#39;Probability&#39;) 5.5.2.0.5 PDF or PMF The Poisson PMF is: \\[P(X=k \\vert \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\] 5.5.2.0.6 Examples The Poisson distribution might be used to model any response variable that is comprised of counts, for example, the number of birds sighted in a bird survey, or the number of people admitted to an emergency room each hour. 5.5.3 Negative Binomial There are two versions or “types” of this distribution, cleverly known as NB1 (type 1) and NB2 (type 2). NB1 has “constant overdispersion” – the variance of the distribution is greater than the mean according to a constant ratio. NB2 has “variable overdispersion” – the variance is a quadratic function of the mean. The NB2 is the one that corresponds directly to conceptualization in terms of binomial trials (with the PMF giving the probability of observing \\(y\\) failures before the \\(r\\)th success). Hardin and Hilbe 2007 describe the negative binomial this way: “Instead of counts entering uniformly, we see counts entering with a specific gamma-distributed shape.” 5.5.3.0.1 Type Discrete 5.5.3.0.2 Support The support is 0 and positive integers (i.e., this distribution works well for count data). It also has a derivation in terms of binomial trials, but in our regression models, we will only use it with count data. 5.5.3.0.3 Parameters A common parameterization of the negative binomial (online and in actuarial science) has parameters \\(p\\), the probability of success on each binomial trial, and \\(r\\), the number of failures observed. The PMF then gives the probability of observing \\(k\\) failures before the \\(r\\)th success in a series of Bernoulli trials. Here, we will use an alternate parameterization. The other common way to parameterize and derive the NB is as a Poisson-gamma mixture – a modified version of a Poisson distribution. In this scheme, the parameters of the distribution are \\(\\mu\\) and \\(\\alpha\\). 5.5.3.0.4 Shapes These distributions can take on unimodal shapes with varying amounts of right skew. In NB2 (type 2) distributions the variance (spread) is larger relative to the mean. NB1: require(gamlss.dist) colrs &lt;- RColorBrewer::brewer.pal(8, &#39;Set2&#39;) gf_dist(&#39;NBI&#39;, params=c(mu=1, sigma=0.5), color=colrs[1]) %&gt;% gf_dist(&#39;NBI&#39;, params=c(mu=1, sigma=0.5), color=colrs[2]) %&gt;% gf_dist(&#39;NBI&#39;, params=c(mu=4, sigma=1), color=colrs[3]) %&gt;% gf_dist(&#39;NBI&#39;, params=c(mu=15, sigma=4), color=colrs[4]) %&gt;% gf_lims(x=c(0,20)) %&gt;% gf_labs(x=&#39;k (number of events)&#39;, y=&#39;Probability&#39;) NB2: colrs &lt;- RColorBrewer::brewer.pal(8, &#39;Set2&#39;) gf_dist(&#39;NBII&#39;, params=c(mu=1, sigma=0.5), color=colrs[1]) %&gt;% gf_dist(&#39;NBII&#39;, params=c(mu=1, sigma=0.5), color=colrs[2]) %&gt;% gf_dist(&#39;NBII&#39;, params=c(mu=4, sigma=1), color=colrs[3]) %&gt;% gf_dist(&#39;NBII&#39;, params=c(mu=15, sigma=4), color=colrs[4]) %&gt;% gf_lims(x=c(0,20)) %&gt;% gf_labs(x=&#39;k (number of events)&#39;, y=&#39;Probability&#39;) 5.5.3.0.5 PDF or PMF Details of the parameterizations and likelihood and fitting of NB1 and NB2 distributions can be found in Hardin and Hilbe 2007, if you are interested. The PMF for the NB1, where the variance is a constant multiple of the mean, is: \\[ f(x \\vert \\mu, \\alpha) = \\frac{\\Gamma(x + \\mu)}{\\Gamma(\\mu)\\Gamma(x+1)}(\\frac{1}{1+\\alpha})^\\mu(\\frac{\\alpha}{1+\\alpha})^x \\] Where \\(\\Gamma\\) is a Gamma function. Note that if \\(\\alpha\\) = 0 this becomes a Poisson distribution, so the Poisson is a special case of the NB1. The PMF for the NB2, where the variance is a quadratic function of the mean, is: \\[ f(x \\vert \\mu, \\alpha) = \\frac{\\Gamma(x + \\frac{1}{\\alpha})}{\\Gamma(\\frac{1}{\\alpha})\\Gamma(x+1)}(\\frac{1}{1+\\alpha \\mu})^{\\frac{1}{\\alpha}}(1 - \\frac{1}{1+\\alpha \\mu})^x \\] 5.5.3.0.6 Examples NB distributions are good models for overdispersed count data, where (in the regression context) the residual variance is not equal to the expected (predicted) value. (Note that if you are reading this before learning about regression models for count data, you may not understand this sentence yet…don’t worry, it will make sense when you return later!) Some examples might include sightings data on numbers of animals seen on wildlife surveys, or the number of items bought per order at an online retailer. 5.5.4 A Mixture Distribution: Tweedie The Tweedie family of distributions is a very large one - depending on the values of the different parameters, the PMF/PDF can be written in many different ways, and it can take on many different shapes. The description below is a simplified one, geared toward the types of Tweedie distributions we are likely to try to use in regression models in this course – mainly the “compound Poisson-gamma” type. Some extra resources for which you will not be held responsible in this course: You can find an accessible description and example of this kind of distribution at: http://www.notenoughthoughts.net/posts/modeling-activity.html. The following site may also be useful in regard to using the Tweedie in regression models: http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_genmod_details28.htm 5.5.4.0.1 Type These distributions are both continuous and discrete - a kind of mix of a Poisson distribution and gamma distribution(s). 5.5.4.0.2 Support The support is non-negative real numbers (greater than or equal to 0). 5.5.4.0.3 Parameters (Note: there are multiple different ways to parameterize these distributions.) We will use: \\(p\\), the index or power parameter, which can be 0 (resulting in a normal distribution), 1 (resulting in a Poisson distribution), \\(1 &lt; p &lt; 2\\) (a compound Poisson-gamma distribution – what we will mainly use), 2 (a gamma distribution), 3 (an inverse Gaussian distribution), \\(&lt;3\\) (a positive stable distribution), or \\(\\infty\\) (an extreme positive stable distribuiton). For the case where \\(1 &lt; p &lt; 2\\), and the distribution is a compound of a Poisson and a gamma, then \\(p = \\frac{k+2}{k+1}\\) where \\(k\\) is the parameter of the gamma distribution. When \\(1&lt;p&lt;2\\), \\(p\\) closer to 1 means thant the Poisson distribution (the mass at 0) gets more “weight” in the compound distribution, and values of \\(p\\) closer to 2 mean that the gamma distribution gets more “weight.” \\(\\mu\\). For the case where \\(1 &lt; p &lt; 2\\), and the distribution is a compound of a Poisson and a gamma, then \\(\\mu = \\lambda k \\theta\\) where \\(\\lambda\\) is the parameter of the Poisson distribution and \\(k\\) and \\(\\theta\\) are the parameters of the gamma. \\(\\phi\\) For the case where \\(1 &lt; p &lt; 2\\), and the distribution is a compound of a Poisson and a gamma, then \\(\\phi = \\frac{\\lambda^{(1-p)} (k \\theta) ^{(2-p)}}{2-p}\\) where \\(\\lambda\\) is the parameter of the Poisson distribution and \\(k\\) and \\(\\theta\\) are the parameters of the gamma. 5.5.4.0.4 Shapes A compound Poisson-gamma Tweedie distribution can take on varying shapes; the main characteristic of interest for us is that it can have a mass at 0, then a unimodal or multimodal distribution with a long right tail (lots of right skew). require(tweedie) tex &lt;- data.frame(x=seq(from=0, by=0.1, to=50)) %&gt;% mutate(dens.a = dtweedie(x, xi = 1.1, mu=4, phi=2)) %&gt;% mutate(dens.b = dtweedie(x, xi = 1.3, mu=4, phi=0.5)) %&gt;% mutate(dens.c = dtweedie(x, xi = 1.5, mu=4, phi=0.3)) %&gt;% mutate(dens.d = dtweedie(x, xi = 1.8, mu=4, phi=1)) %&gt;% mutate(dens.e = dtweedie(x, xi = 1.5, mu=4, phi=5)) gf_line(dens.a ~ x, data=tex, color=colrs[1]) %&gt;% gf_line(dens.b ~x, data=tex, color=colrs[2]) %&gt;% gf_line(dens.c ~x, data=tex, color=colrs[3]) %&gt;% gf_line(dens.d ~x, data=tex, color=colrs[4]) %&gt;% gf_line(dens.e ~x, data=tex, color=colrs[5]) 5.5.4.0.5 PDF or PMF A Tweedie distribution with \\(p&gt;1\\) has the form: \\[ f(X \\vert \\mu, \\phi, p) = a(x, \\phi)e^{\\frac{1}{\\phi}(\\frac{x\\mu^{(1-p)}}{(1-p)} - \\kappa(\\mu,p))}\\] where \\(\\kappa(\\mu,p) = \\frac{\\mu^{(2-p)}}{(2-p)}\\) if \\(p\\neq2\\), and if \\(p=2\\), \\(\\kappa(\\mu,p) = log(\\mu)\\); but \\(a(x,\\phi)\\) is a function that does not have an analytical expression. This expression is from SAS documentation at https://support.sas.com/rnd/app/stat/examples/tweedie/tweedie.pdf. Alternately (and more simply(?)), a Tweedie distribution with \\(1 &lt; p &lt; 2\\) is a compound of a Poisson distribution with parameter \\(\\lambda\\) and a gamma distribution with parameters \\(k\\) and \\(\\theta\\). For example: “Suppose that airplanes arrive at an airport following a Poisson process, and the number of passengers in each airplane follows a certain gamma distribution. Then, the number of passengers arriving at the airport follows a compound Poisson gamma process \\[ Y = \\sum_{i=1}^{N} D_i\\] where \\(N\\) is the Poisson process that the airplanes follow, and \\(D_i\\) is the gamma distribution that the passengers follow.” (Thanks to D. Mao, http://math.uchicago.edu/~may/REU2013/REUPapers/Mao.pdf for this example.) 5.5.4.0.6 Examples The Tweedie distributions may be useful for “zero-inflated” data, where there is a class of observations for which the observed value of the variable is always zero, and another class for which the variable takes on positive continuous values. For example, this might model the number of birds present per unit area (when the study area includes places of unsuitable habitat where none are ever found), or perhaps the quantity of alcohol consumed per week by different people (some of whom may drink varying amounts, and others of whom may never drink at all). "],
["regression-for-count-data.html", "Chapter 6 Regression for Count Data 6.1 Data Source 6.2 A bad idea: multiple linear regression model 6.3 Problems with the linear model 6.4 Poisson Regression 6.5 Accounting for overdispersion: Negative Binomial Models 6.6 Accounting for overdispersion: quasi-Poisson Model 6.7 Model selection with dredge() and (Q)AIC, BIC 6.8 Offsets 6.9 Prediction Plots", " Chapter 6 Regression for Count Data So far, we have fitted regression models for continuous-valued quantitative response variables. What if our response variable is really count data – discrete quantitative values limited to zero and positive integers? 6.1 Data Source The dataset used here is beevisits, from: Felicity Muth, Jacob S. Francis, and Anne S. Leonard. 2017. Bees use the taste of pollen to determine which flowers to visit. Biology Letters 12(7): DOI: 10.1098/rsbl.2016.0356. http://rsbl.royalsocietypublishing.org/content/roybiolett/12/7/20160356.full.pdf. knitr::include_graphics(&#39;images/BeeExperiment.png&#39;) 6.2 A bad idea: multiple linear regression model summary(beevisits) ## colony beename treatment target.colour ## W:126 giantbeeYsucroseyellow: 3 Quinine :90 blue :138 ## X: 66 o51Wsucroseblue : 3 Cellulose:90 yellow:132 ## Y: 78 o54Wquinineblue : 3 Sucrose :90 ## o58Wcelluloseyellow : 3 ## o60Wcelluloseblue : 3 ## o63Wquinineblue : 3 ## (Other) :252 ## novisits flower ## Min. : 0.000 familiar:90 ## 1st Qu.: 1.000 novel :90 ## Median : 3.000 target :90 ## Mean : 4.437 ## 3rd Qu.: 7.000 ## Max. :21.000 ## bee.lm &lt;- lm(novisits ~ flower + treatment + colony, data=beevisits) summary(bee.lm) ## ## Call: ## lm(formula = novisits ~ flower + treatment + colony, data = beevisits) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5668 -2.1113 -0.2847 1.5665 11.5767 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1113 0.4440 4.755 3.27e-06 *** ## flowernovel -1.6778 0.4386 -3.825 0.000163 *** ## flowertarget 5.4556 0.4386 12.438 &lt; 2e-16 *** ## treatmentCellulose 2.6883 0.4389 6.124 3.30e-09 *** ## treatmentSucrose 2.7240 0.4415 6.170 2.56e-09 *** ## colonyX -0.8318 0.4491 -1.852 0.065110 . ## colonyY -1.8493 0.4254 -4.347 1.97e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.942 on 263 degrees of freedom ## Multiple R-squared: 0.5763, Adjusted R-squared: 0.5667 ## F-statistic: 59.63 on 6 and 263 DF, p-value: &lt; 2.2e-16 6.3 Problems with the linear model What problems do we have with this model (its appropriateness, or goodness of fit to the data)? Non-constant error variance Non-normality of residuals Some predicted values are less than 0 – it’s impossible that a bee could visit a flower a negative number of times! 6.4 Poisson Regression Detailed notes on the model equation were filled in here in class 6.4.1 Fitting the Model prm &lt;- glm(novisits ~ flower + treatment + colony, data=beevisits, family=poisson(link=&#39;log&#39;)) summary(prm) ## ## Call: ## glm(formula = novisits ~ flower + treatment + colony, family = poisson(link = &quot;log&quot;), ## data = beevisits) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9940 -1.3124 -0.3860 0.7794 4.7020 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.79055 0.08695 9.092 &lt; 2e-16 *** ## flowernovel -0.75072 0.10442 -7.189 6.51e-13 *** ## flowertarget 0.99945 0.06916 14.451 &lt; 2e-16 *** ## treatmentCellulose 0.69817 0.07910 8.827 &lt; 2e-16 *** ## treatmentSucrose 0.70953 0.07967 8.906 &lt; 2e-16 *** ## colonyX -0.17521 0.07175 -2.442 0.0146 * ## colonyY -0.43724 0.07311 -5.981 2.22e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1236.33 on 269 degrees of freedom ## Residual deviance: 542.05 on 263 degrees of freedom ## AIC: 1260.6 ## ## Number of Fisher Scoring iterations: 5 6.4.2 Conditions What conditions must hold for this model to be appropriate? Response variable (y) contains count data Linearity: \\(log(\\lambda_{i})\\) is a linear function of the covariates \\(x_1\\), \\(x_2\\), … \\(x_n\\). (Later we will consider how to check this when some covariates are quantitative…in brief: plot log(rate) as a function of each covariate and look for (no non-)linear trend.) Mean = Variance Independence (of residuals) There is not a condition specifying a PDF that the residuals should follow. 6.4.3 Model Assessment Which conditions can we check already? acf(resid(prm, type=&#39;response&#39;), main=&#39;bee.lm Residuals&#39;) What does type='response' mean? This means that the residuals are reported on the same scale as the response variable (that is, in units of counts, rather than log(counts)). beevisits &lt;- beevisits %&gt;% mutate(pm.resids = resid(prm, type=&#39;response&#39;), pm.fitted = fitted(prm)) gf_point(pm.resids ~ pm.fitted, data=beevisits) %&gt;% gf_labs(x=&#39;Fitted Values&#39;, y=&#39;Residuals&#39;) This trumpet pattern is what we expect! Why? If the variance equals the mean, then as the mean (fitted value) goes up, then the variance (spread of residuals) will also be larger. But how can we decide if it’s the right amount of increase in variance with increase in fitted value? We can compute Pearson residuals, which are scaled by the expected variance. The Pearson residuals should have approximately constant variance as a function of fitted values, and a standard deviation of about 1. beevisits &lt;- beevisits %&gt;% mutate(pm.pearson.resids = resid(prm, type=&#39;pearson&#39;)) gf_point( pm.pearson.resids ~ pm.fitted, data=beevisits) %&gt;% gf_labs(x=&#39;Fitted Values&#39;, y=&#39;Pearson Residuals&#39;) A more complex solution: we could divide the fitted values into bins (choice of bin size is somewhat arbitrary; generally, you want them as small as possible, but still containing enough observations per bin to get good mean and variance estimates). In each bin, compute the mean and the variance of the residuals. Plot these means vs these variances and see if the slope is 1 (and intercept 0). First, split the fitted values into 5 bins: beevisits &lt;- beevisits %&gt;% mutate(fitted.bins = cut(fitted(prm), breaks=5)) head(beevisits) ## colony beename treatment target.colour novisits flower ## 1 Y giantbeeYsucroseyellow Sucrose yellow 12 target ## 2 W o51Wsucroseblue Sucrose blue 8 target ## 3 W o54Wquinineblue Quinine blue 13 target ## 4 W o58Wcelluloseyellow Cellulose yellow 9 target ## 5 W o60Wcelluloseblue Cellulose blue 11 target ## 6 W o63Wquinineblue Quinine blue 5 target ## lm.resids lm.fitted pm.resids pm.fitted pm.pearson.resids fitted.bins ## 1 3.5585187 8.441481 4.1360526 7.863947 1.4749108 (7.57,9.88] ## 2 -2.2907806 10.290781 -4.1767222 12.176722 -1.1969345 (9.88,12.2] ## 3 5.4331935 7.566806 7.0105824 5.989418 2.8645856 (5.27,7.57] ## 4 -1.2551114 10.255111 -3.0392024 12.039202 -0.8759126 (9.88,12.2] ## 5 0.7448886 10.255111 -1.0392024 12.039202 -0.2995031 (9.88,12.2] ## 6 -2.5668065 7.566806 -0.9894176 5.989418 -0.4042847 (5.27,7.57] Next, compute the means and variances in each bin, view the result, and plot: binned.bees &lt;- beevisits %&gt;% group_by(fitted.bins) %&gt;% summarise(mean = mean(pm.fitted, na.rm=TRUE), var = var(pm.resids)) binned.bees ## # A tibble: 5 x 3 ## fitted.bins mean var ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (0.661,2.97] 1.77 3.34 ## 2 (2.97,5.27] 4.28 10.6 ## 3 (5.27,7.57] 5.99 21.3 ## 4 (7.57,9.88] 7.83 12.8 ## 5 (9.88,12.2] 11.5 13.5 gf_point(var ~ mean, data=binned.bees) %&gt;% gf_labs(x=&#39;Mean Fitted Value&#39;, y=&#39;Variance of Residuals&#39;) %&gt;% gf_abline(slope=1, intercept=0) That’s a bit of a pain, and still a judgment call at the end. How else can we check the Mean = Variance condition? 6.4.4 Checking for overdispersion using overdispersion factor We can estimate the overdispersion factor. This satisfies \\[ \\text{residual variance} = \\text{overdispersion factor} * \\text{mean}\\] So if it’s a lot larger than 1, we have a problem. The function overdisp_fun() in package s245 computes an estimate: require(s245) overdisp_fun(prm) ## [1] 2.047036 Here, it’s about 2: not too terrible, but still, residual variance is double what it should be according to our model. (Usually if the overdispersion is larger than 2 we will prefer a different model that better accounts for the fact that \\(\\text{mean} \\neq \\text{variance}\\).) 6.5 Accounting for overdispersion: Negative Binomial Models Finally, we could simply fit a model that allows for a more permissive mean-variance relationship (where the variance can be larger or smaller than the mean, by different amounts), and see if it is a better fit to the data according it our IC. The negative binomial distributions (type I and II) do this: require(glmmTMB) nbm1 &lt;- glmmTMB(novisits ~ flower + treatment + colony, data=beevisits, family=nbinom1(link=&#39;log&#39;)) nbm2 &lt;- glmmTMB(novisits ~ flower + treatment + colony, data=beevisits, family=nbinom2(link=&#39;log&#39;)) summary(nbm1) ## Family: nbinom1 ( log ) ## Formula: novisits ~ flower + treatment + colony ## Data: beevisits ## ## AIC BIC logLik deviance df.resid ## 1182.6 1211.4 -583.3 1166.6 262 ## ## ## Overdispersion parameter for nbinom1 family (): 1.07 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.71730 0.12287 5.838 5.28e-09 *** ## flowernovel -0.74144 0.14503 -5.112 3.18e-07 *** ## flowertarget 1.03838 0.09810 10.585 &lt; 2e-16 *** ## treatmentCellulose 0.71563 0.11167 6.408 1.47e-10 *** ## treatmentSucrose 0.74317 0.11149 6.666 2.63e-11 *** ## colonyX -0.11496 0.09974 -1.153 0.249038 ## colonyY -0.38387 0.10155 -3.780 0.000157 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(nbm2) ## Family: nbinom2 ( log ) ## Formula: novisits ~ flower + treatment + colony ## Data: beevisits ## ## AIC BIC logLik deviance df.resid ## 1207.3 1236.1 -595.6 1191.3 262 ## ## ## Overdispersion parameter for nbinom2 family (): 4.69 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.73536 0.11775 6.245 4.24e-10 *** ## flowernovel -0.72548 0.12693 -5.716 1.09e-08 *** ## flowertarget 1.04379 0.09983 10.456 &lt; 2e-16 *** ## treatmentCellulose 0.76192 0.11229 6.785 1.16e-11 *** ## treatmentSucrose 0.76936 0.11424 6.734 1.65e-11 *** ## colonyX -0.16805 0.10838 -1.551 0.121 ## colonyY -0.51527 0.10761 -4.788 1.68e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can use AIC or BIC-based model selection to decide which model fits best, between Poisson, NB1, and NB2 models. AIC(prm, nbm1, nbm2) ## df AIC ## prm 7 1260.621 ## nbm1 8 1182.575 ## nbm2 8 1207.286 We have a clear winner: NB1! 6.6 Accounting for overdispersion: quasi-Poisson Model Or yet another option…in our class, we will not use quasi-Poisson models as much, as they are fitted via quasi-likelihood and this complicates model selection. qprm &lt;- glm(novisits ~ flower + treatment + colony, data=beevisits, family=quasipoisson(link=&#39;log&#39;)) summary(qprm) ## ## Call: ## glm(formula = novisits ~ flower + treatment + colony, family = quasipoisson(link = &quot;log&quot;), ## data = beevisits) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9940 -1.3124 -0.3860 0.7794 4.7020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.79055 0.12440 6.355 9.14e-10 *** ## flowernovel -0.75072 0.14940 -5.025 9.32e-07 *** ## flowertarget 0.99945 0.09896 10.100 &lt; 2e-16 *** ## treatmentCellulose 0.69817 0.11317 6.169 2.58e-09 *** ## treatmentSucrose 0.70953 0.11399 6.225 1.90e-09 *** ## colonyX -0.17521 0.10266 -1.707 0.0891 . ## colonyY -0.43724 0.10460 -4.180 3.97e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.047071) ## ## Null deviance: 1236.33 on 269 degrees of freedom ## Residual deviance: 542.05 on 263 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 We should not compare AIC with QAIC to compare two models, so to decide between Poisson and quasi-Poisson (if you ever use it) you must rely on the model assessment plots and the overdispersion factor to decide which is better. 6.7 Model selection with dredge() and (Q)AIC, BIC Poisson and negative binomial models are fitted via maximum likelihood, so AIC or BIC may be used for model selection. How can we use model selection criteria in a case where the likelihood can’t be computed exactly? The quasi (log) likelihood is an approximation to the likelihood. It has some properties in common with a (log) likelihood, but is not a likelihood; we resort to using it in cases where the (log) likelihood can not even be evaluated. With some code/mathematical gymnastics, we can use principles of quasi-likelihood to estimate QAIC (quasi-AIC, or AIC based on quasi-likelihood) in R for model selection. 6.7.1 Review: all subsets selection with dredge() What if, instead of comparing two (or a few) specific models, we want to compare all possible models that contain some combination of a set of candidate covariates? The package MuMIn contains a function, dredge(), that takes as input a “full” model (one containing all the candidate covariates). It then computes AIC (or other model selection criteria) for all possible models containing subsets of the covariate and reports the results in a ranked table. For example, for our Poisson regression model, we could do: require(MuMIn) #have to make sure na.action is &#39;na.fail&#39; for input model prm &lt;- update(prm, na.action=&#39;na.fail&#39;) dredge(prm, rank=&#39;AIC&#39;) ## Global model call: glm(formula = novisits ~ flower + treatment + colony, family = poisson(link = &quot;log&quot;), ## data = beevisits, na.action = &quot;na.fail&quot;) ## --- ## Model selection table ## (Intrc) colny flowr trtmn df logLik AIC delta weight ## 8 0.7905 + + + 7 -623.310 1260.6 0.00 1 ## 7 0.6428 + + 5 -642.317 1294.6 34.01 0 ## 4 1.3100 + + 5 -677.205 1364.4 103.79 0 ## 3 1.1560 + 3 -695.120 1396.2 135.62 0 ## 6 1.1240 + + 5 -898.637 1807.3 546.65 0 ## 5 0.9767 + 3 -917.644 1841.3 580.67 0 ## 2 1.6440 + 3 -952.532 1911.1 650.44 0 ## 1 1.4900 1 -970.446 1942.9 682.27 0 ## Models ranked by AIC(x) dredge(prm, rank=&#39;BIC&#39;) ## Global model call: glm(formula = novisits ~ flower + treatment + colony, family = poisson(link = &quot;log&quot;), ## data = beevisits, na.action = &quot;na.fail&quot;) ## --- ## Model selection table ## (Intrc) colny flowr trtmn df logLik BIC delta weight ## 8 0.7905 + + + 7 -623.310 1285.8 0.00 1 ## 7 0.6428 + + 5 -642.317 1312.6 26.82 0 ## 4 1.3100 + + 5 -677.205 1382.4 96.59 0 ## 3 1.1560 + 3 -695.120 1407.0 121.23 0 ## 6 1.1240 + + 5 -898.637 1825.3 539.46 0 ## 5 0.9767 + 3 -917.644 1852.1 566.27 0 ## 2 1.6440 + 3 -952.532 1921.9 636.05 0 ## 1 1.4900 1 -970.446 1946.5 660.68 0 ## Models ranked by BIC(x) 6.7.2 Review: IC “weights” Note the last two columns of the dredge output: the “delta” (or \\(\\delta\\)) AIC or BIC values and the “weights”. The \\(\\delta\\)s are obtained by simply subtracting the best model’s IC value from that of each other model. We already mentioned a rule of thumb: that the \\(\\delta IC\\) should be at least 3 or to provide reasonable evidence that one model is really better than the other. Another way of measuring the differences between models is to use model weights. Theoretically, these measure the relative likelihoods of different models; you can think of them as giving the probability that a given model is the best-fitting one in the set of models examined, according to the IC being used. Model weights are computed simply according to: \\[ e^\\frac{-\\delta IC}{2}\\] And they sum to one for all models in a dredge(). 6.7.3 Extending dredge() to quasi-Likelihood With (rather a lot of) effort to define some custom functions, we can do the same thing for a quasi-Poisson model using quasi-AIC. Note: This idea is based on notes by Ben Bolker provided with the R package bbmle. require(MuMIn) # modify a glm() output object so that # it contains a quasipoisson fit but the # AIC (likelihood) from the equivalent regular Poisson model x.quasipoisson &lt;- function(...) { res &lt;- quasipoisson(...) res$aic &lt;- poisson(...)$aic res } # function to extract the overdispersion parameter # from a quasi model dfun &lt;- function(object) { with(object,sum((weights * residuals^2)[weights &gt; 0])/df.residual) } # function that modifies MuMIn::dredge() # for use with quasi GLM qdredge &lt;- function(model, family=&#39;x.quasipoisson&#39;, na.action=na.fail, chat = dfun(model), rank=&#39;QAIC&#39;, ...){ model2 &lt;- update(model, family=family, na.action=na.action) (dt &lt;- dredge(model2, rank=rank, chat=chat, ...)) } #do &quot;dredge&quot; for model selection qdredge(qprm) ## Global model call: glm(formula = novisits ~ flower + treatment + colony, family = family, ## data = beevisits, na.action = na.action) ## --- ## Model selection table ## (Intrc) colny flowr trtmn df logLik rank delta weight ## 8 0.7905 + + + 7 -623.310 625.0 0.00 0.999 ## 7 0.6428 + + 5 -642.317 639.5 14.57 0.001 ## 4 1.3100 + + 5 -677.205 673.6 48.66 0.000 ## 3 1.1560 + 3 -695.120 687.1 62.16 0.000 ## 6 1.1240 + + 5 -898.637 890.0 265.00 0.000 ## 5 0.9767 + 3 -917.644 904.5 279.57 0.000 ## 2 1.6440 + 3 -952.532 938.6 313.65 0.000 ## 1 1.4900 1 -970.446 952.1 327.15 0.000 ## Models ranked by rank(x, chat = 2.04707063774377) Note: the qdredge() function is also provided for you in the package s245. So if you do want to use this, all you need to do is use s245::qdredge() instead of dredge(). 6.8 Offsets In our model, the response variable was the number of flowers visited by a bee, and each bee was in the same experimental setting for the same amount of time, so there was no need to account for effort or time spent in each case. This is not always true: consider, for example: Dolphin surveys Bird counts Consider the schools and crime example from homework as well In this case, it would be natural to adjust for effort. The intuitive way to do it would be to use counts per unit effort as the response variable: \\[ log(\\frac{\\lambda_i}{effort}) = \\beta_0 + \\dots \\] But notice that this is equivalent to including \\(log(effort)\\) as an ``offset\" on the right hand side of the regression equation: \\[ log(\\lambda_i) = \\beta_0 + \\dots + log(effort)\\] This is how we specify models with offsets in R: offset.mod &lt;- glm(counts ~ predictor1 + predictor2 + offset(log(effort)), family=poisson) Note: if you use dredge() with a model with an offset, be sure to specify the offset as a “fixed” term, i.e. a term that must be included in all models: dredge(offset.mod, fixed = &#39;offset(log(effort))&#39;) 6.9 Prediction Plots Once you have a “best” model, how do you interpret it? When we went to great lengths to make prediction plots for a linear regression model so we could “see” the slope of the predicted relationship, you may have wondered: why bother? I can just look at the slope estimate and get the same insight! Now, with a more complicated model equation with a link function, it’s not so easy. Now, we will really appreciate those prediction plots! With the link function and the Poisson distribution, it is more challenging to interpret the coefficients of the model directly. The easiest way to understand the effects of different predictors is to look at plots of model predictions. However, as always we don’t want to plot fitted(model) as a function of each predictor in a model with more than one predictor; predictors other than the one we are interested in will influence the predictions, introducing extra variation. Instead, we will construct a new (fake) dataset to make predictions for, in which all predictors but the one we are interested in are held constant. For example, using our quasi-Poisson model and looking at the effect of colony (predicted values with 95% CIs): newdata &lt;- data.frame(colony=c(&#39;W&#39;, &#39;X&#39;, &#39;Y&#39;), flower=&#39;familiar&#39;, target.colour=&#39;blue&#39;, treatment=&#39;Cellulose&#39;) pred = predict(qprm, newdata=newdata, type=&#39;response&#39;, se.fit=TRUE) newdata &lt;- newdata %&gt;% mutate(preds = pred$fit, CIlow = pred$fit - 1.96*pred$se.fit, CIup = pred$fit + 1.96*pred$se.fit) gf_point(preds ~ colony, data=newdata) %&gt;% gf_labs(x=&#39;Colony&#39;, y=&#39;Predicted\\nN. Visits&#39;) %&gt;% gf_errorbar(CIlow + CIup ~ colony, data=newdata) If we had a quantitative predictor instead, the process would be similar, except when we define newdata, we would have to choose a range and granularity for which to make predictions. For example, imagine if bee length in mm was one of our predictors, and we wanted predictions for lengths between 0.5 and 3 mm (with a value every 0.05mm). We might begin: newdata &lt;- expand.grid(bee.length = seq(from=0.05, by=0.05, to=3), colony=c(&#39;W&#39;), flower=&#39;familiar&#39;, target.colour=&#39;blue&#39;, treatment=&#39;Cellulose&#39;) head(newdata) ## bee.length colony flower target.colour treatment ## 1 0.05 W familiar blue Cellulose ## 2 0.10 W familiar blue Cellulose ## 3 0.15 W familiar blue Cellulose ## 4 0.20 W familiar blue Cellulose ## 5 0.25 W familiar blue Cellulose ## 6 0.30 W familiar blue Cellulose Then, when we make the plot, we would need to use gf\\_ribbon() instead of gf\\_errorbar() to show the CI. We can also use pred_plot(fitted_model, 'variable_name') to make these plots with a lot less code (and if you need to know the values at which other predictors are fixed, use get_fixed(dataset)). "],
["model-averaging.html", "Chapter 7 Model Averaging 7.1 Data: School Survey on Crime and Safety 7.2 Modelling number of violent incidents per school 7.3 Model Averaging", " Chapter 7 Model Averaging So far, we have used AIC and/or BIC for model selection, to decide which variables to keep in a “best” model and which to exclude. But we have already seen a number of cases where there is not one model that is clearly superior to all the others. In those cases, we have decided on the smallest model (with the fewest predictors), but there are other options when many competing models have similar scores. One option is not to choose – instead, keep them all, and make predictions via a weighted average of all the models. The models with the best IC scores get more weight. This can be a good option if the main goal is accurate prediction (rather than deciding definitively which predictors are “good” ones and which are not). How can we do it? Let’s explore an example. 7.1 Data: School Survey on Crime and Safety The data for this example are from a survey of U.S. schools, the 2000 School Survey on Crime and Safety. There is information about the study at http://catalog.data.gov/dataset/2000-school-survey-on-crime-and-safety, which says the study “is a cross-sectional survey of the nation’s public schools designed to provide estimates of school crime, discipline, disorder, programs and policies. SSOCS is administered to public primary, middle, high, and combined school principals in the spring of even-numbered school years…Public schools were sampled in the spring of 2000 to participate in the study.” The dataset you will use is available online at: http://sldr.netlify.com/data/sscrime.csv It contains a number of variables: VisitorCheckIn: Whether visitors to the school must check in to gain entry to the school. LockedGates: Whether there are locked gates at the entry to the school. MetalDetectors: Whether there is a metal detector at the entrance to the school. DrugSniffDog: Whether a drug-sniffing dog is randomly brought into the school to carry out inspections. DrugTesting: Whether any drug testing of students occurs. UniformsRequired:Whether students are required to wear uniforms. DressCode: Whether a strict dress code is enforced. Lockers: Whether students have lockers. StudentIDBadges: Whether students are required to wear ID badges. StaffIDBadges: Whether teachers and other staff are required to wear ID badges. SecurityCameras: Whether there are security cameras on the premises. OfficialRiotPlan: Whether the school has a written plan in place for how to deal with a riot or large-scale fight. ViolenceReductionProgram: Whether the school has a Violence Reduction Program in place. Security: Whether security officers are present on the premises. TrainingHours: Average amount of time (in hours) that teachers and staff have devoted to training related to violence reduction. AttacksWithoutWeapon: Number of attacks that have occurred at the school, not involving a weapon. Thefts: Number of thefts. Vandalism: Number of incidents of vandalism. ViolentIncidentsTotal: Number of violent incidents of all types that have occurred at the school. Enrollment: Number of students enrolled in the school (categorical) NEnrollment: Number of students enrolled in the school (numeric) SurveyRespondent: The identity of the person who filled out the survey. Location: Whether the location of the school is Urban, Rural, etc. ssc &lt;- read.csv(&#39;http://sldr.netlify.com/data/sscrime.csv&#39;) 7.2 Modelling number of violent incidents per school We will fit a model for the number of violent incidents total as a function of a number of predictors. This is count data and we will fit a negative binomial regression model: require(glmmTMB) school.nb2 &lt;- glmmTMB(ViolentIncidentsTotal ~ TrainingHours + Location + SecurityCameras + DressCode + UniformsRequired + NEnrollment, data=ssc, family=nbinom2(link=&#39;log&#39;)) I will use AIC and the dredge() function to compare all possible subsets of my saturated model and figure out which variables should be included in the best model. I chose AIC in this case because it is perhaps more widely used than BIC (that’s not a good reason unless you really have no better one, but there you have it) and because with the relatively small sample size here, I don’t feel a particular need to use BIC for its larger penalty term. require(MuMIn) #do &quot;dredge&quot; for model selection mod.sel &lt;- dredge(school.nb2, rank=&#39;AIC&#39;) head(mod.sel, 8) ## Global model call: glmmTMB(formula = ViolentIncidentsTotal ~ TrainingHours + Location + ## SecurityCameras + DressCode + UniformsRequired + NEnrollment, ## data = ssc, family = nbinom2(link = &quot;log&quot;), ziformula = ~0, ## dispformula = ~1) ## --- ## Model selection table ## cnd((Int)) dsp((Int)) cnd(DrC) cnd(Lct) cnd(NEn) cnd(ScC) cnd(TrH) ## 4 3.610 + + + ## 36 3.585 + + + ## 12 3.635 + + + + ## 8 3.570 + + + 2.537e-05 ## 20 3.608 + + + 0.0008916 ## 44 3.610 + + + + ## 40 3.529 + + + 3.410e-05 ## 16 3.588 + + + 3.140e-05 + ## cnd(UnR) df logLik AIC delta weight ## 4 6 -1745.380 3502.8 0.00 0.287 ## 36 + 7 -1745.017 3504.0 1.27 0.152 ## 12 7 -1745.103 3504.2 1.45 0.139 ## 8 7 -1745.280 3504.6 1.80 0.117 ## 20 7 -1745.380 3504.8 2.00 0.106 ## 44 + 8 -1744.734 3505.5 2.71 0.074 ## 40 + 8 -1744.840 3505.7 2.92 0.067 ## 16 8 -1744.953 3505.9 3.15 0.060 ## Models ranked by AIC(x) Because the first 7 or so models all have AIC scores within 3 units of each other, it is hard to choose one best model here. In this situation, one way to choose is to pick the model that includes the smallest number of predictors, and still acheives an AIC that is among the best. Another option would be to use model averaging. 7.3 Model Averaging What if we wanted to use model averaging to find the best model, instead? We might choose this route because there are several models that all have AIC that are close to each other and thus fit the data approximately equally well. So we might choose to make predictions (and compute coefficients) that are the average of all the models (weighted by IC weights). Notes of caution: If the model is not a linear regression (if there is a link function for instance) then it’s important to get predictions by averaging the predictions from the different models, not by making predictions using the model-averaged coefficients. The code below is careful to do this. Model averaging is used pretty widely but is also controversial (like most model selection methods, in fact!) For example, see: [https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/14-1639.1] and [https://drewtyre.rbind.io/post/rebutting_cade/]. To do model averaging, we use package MuMIn (function model.avg). 7.3.1 Getting the Averaged Model The following code gets the average model. If we did the default (fit=FALSE), it would be a bit faster, but we would then not be able to get predictions from the model. mod.sel2 &lt;- dredge(school.nb2) ave.model &lt;- MuMIn::model.avg(mod.sel2, fit=TRUE) summary(ave.model) ## ## Call: ## model.avg(object = get.models(object = mod.sel2, subset = NA)) ## ## Component model call: ## glmmTMB(formula = ViolentIncidentsTotal ~ &lt;64 unique rhs&gt;, data = ## ssc, family = nbinom2(link = &quot;log&quot;), ziformula = ~0, dispformula = ## ~1) ## ## Component models: ## df logLik AICc delta weight ## 12 6 -1745.38 3502.98 0.00 0.24 ## 126 7 -1745.02 3504.33 1.35 0.12 ## 124 7 -1745.10 3504.51 1.52 0.11 ## 123 7 -1745.28 3504.86 1.88 0.09 ## 125 7 -1745.38 3505.06 2.08 0.08 ## 1246 8 -1744.73 3505.86 2.87 0.06 ## 1236 8 -1744.84 3506.07 3.08 0.05 ## 1234 8 -1744.95 3506.29 3.31 0.05 ## 1256 8 -1745.02 3506.42 3.44 0.04 ## 1245 8 -1745.10 3506.59 3.60 0.04 ## 1235 8 -1745.28 3506.95 3.96 0.03 ## 12346 9 -1744.49 3507.46 4.48 0.03 ## 12456 9 -1744.73 3507.95 4.96 0.02 ## 12356 9 -1744.84 3508.17 5.18 0.02 ## 12345 9 -1744.95 3508.38 5.40 0.02 ## 123456 10 -1744.48 3509.56 6.58 0.01 ## 2 5 -1753.09 3516.33 13.35 0.00 ## 26 6 -1752.29 3516.80 13.82 0.00 ## 24 6 -1752.60 3517.42 14.44 0.00 ## 246 7 -1751.79 3517.87 14.89 0.00 ## 25 6 -1753.09 3518.40 15.41 0.00 ## 23 6 -1753.09 3518.40 15.41 0.00 ## 236 7 -1752.26 3518.82 15.84 0.00 ## 256 7 -1752.29 3518.87 15.89 0.00 ## 234 7 -1752.59 3519.48 16.49 0.00 ## 245 7 -1752.59 3519.49 16.50 0.00 ## 2346 8 -1751.72 3519.82 16.83 0.00 ## 2456 8 -1751.78 3519.96 16.97 0.00 ## 235 7 -1753.09 3520.47 17.49 0.00 ## 2356 8 -1752.26 3520.90 17.92 0.00 ## 2345 8 -1752.59 3521.56 18.57 0.00 ## 23456 9 -1751.71 3521.91 18.92 0.00 ## 136 5 -1756.63 3523.42 20.43 0.00 ## 1346 6 -1756.08 3524.39 21.41 0.00 ## 13 4 -1758.25 3524.60 21.62 0.00 ## 16 4 -1758.59 3525.28 22.29 0.00 ## 1356 6 -1756.61 3525.44 22.45 0.00 ## 134 5 -1757.74 3525.65 22.66 0.00 ## 1 3 -1759.81 3525.68 22.69 0.00 ## 13456 7 -1756.08 3526.46 23.48 0.00 ## 135 5 -1758.21 3526.59 23.60 0.00 ## 146 5 -1758.30 3526.76 23.78 0.00 ## 14 4 -1759.53 3527.16 24.17 0.00 ## 156 5 -1758.54 3527.24 24.26 0.00 ## 15 4 -1759.75 3527.61 24.62 0.00 ## 1345 6 -1757.73 3527.69 24.71 0.00 ## 1456 6 -1758.28 3528.79 25.80 0.00 ## 145 5 -1759.50 3529.15 26.17 0.00 ## 36 4 -1765.82 3539.76 36.77 0.00 ## 346 5 -1764.96 3540.08 37.10 0.00 ## 6 3 -1767.12 3540.31 37.33 0.00 ## 46 4 -1766.54 3541.19 38.20 0.00 ## 356 5 -1765.77 3541.70 38.71 0.00 ## 3456 6 -1764.95 3542.12 39.14 0.00 ## 56 4 -1767.04 3542.18 39.20 0.00 ## (Null) 2 -1769.34 3542.72 39.74 0.00 ## 3 3 -1768.50 3543.05 40.07 0.00 ## 456 5 -1766.50 3543.17 40.18 0.00 ## 34 4 -1767.71 3543.52 40.54 0.00 ## 4 3 -1768.77 3543.61 40.63 0.00 ## 5 3 -1769.24 3544.54 41.55 0.00 ## 35 4 -1768.41 3544.93 41.95 0.00 ## 345 5 -1767.68 3545.51 42.53 0.00 ## 45 4 -1768.72 3545.54 42.55 0.00 ## ## Term codes: ## cond(DressCode) cond(Location) cond(NEnrollment) ## 1 2 3 ## cond(SecurityCameras) cond(TrainingHours) cond(UniformsRequired) ## 4 5 6 ## ## Model-averaged coefficients: ## (full average) ## Estimate Std. Error Adjusted SE z value ## cond((Int)) 3.596e+00 1.316e-01 1.320e-01 27.252 ## cond(DressCodeyes) 3.765e-01 9.608e-02 9.639e-02 3.906 ## cond(LocationRural) -5.597e-01 1.328e-01 1.332e-01 4.201 ## cond(LocationTown) -5.874e-01 1.543e-01 1.548e-01 3.795 ## cond(LocationUrban Fringe) -1.024e-01 1.179e-01 1.183e-01 0.866 ## cond(UniformsRequiredyes) 5.685e-02 1.373e-01 1.376e-01 0.413 ## cond(SecurityCamerasyes) -2.384e-02 6.451e-02 6.466e-02 0.369 ## cond(NEnrollment) 8.826e-06 2.812e-05 2.818e-05 0.313 ## cond(TrainingHours) -2.156e-04 2.509e-02 2.517e-02 0.009 ## Pr(&gt;|z|) ## cond((Int)) &lt; 2e-16 *** ## cond(DressCodeyes) 9.39e-05 *** ## cond(LocationRural) 2.65e-05 *** ## cond(LocationTown) 0.000148 *** ## cond(LocationUrban Fringe) 0.386523 ## cond(UniformsRequiredyes) 0.679432 ## cond(SecurityCamerasyes) 0.712383 ## cond(NEnrollment) 0.754163 ## cond(TrainingHours) 0.993166 ## ## (conditional average) ## Estimate Std. Error Adjusted SE z value ## cond((Int)) 3.596e+00 1.316e-01 1.320e-01 27.252 ## cond(DressCodeyes) 3.771e-01 9.499e-02 9.530e-02 3.956 ## cond(LocationRural) -5.598e-01 1.328e-01 1.332e-01 4.203 ## cond(LocationTown) -5.874e-01 1.542e-01 1.547e-01 3.796 ## cond(LocationUrban Fringe) -1.024e-01 1.179e-01 1.183e-01 0.866 ## cond(UniformsRequiredyes) 1.661e-01 1.921e-01 1.927e-01 0.862 ## cond(SecurityCamerasyes) -7.409e-02 9.597e-02 9.628e-02 0.770 ## cond(NEnrollment) 3.049e-05 4.550e-05 4.565e-05 0.668 ## cond(TrainingHours) -8.265e-04 4.912e-02 4.928e-02 0.017 ## Pr(&gt;|z|) ## cond((Int)) &lt; 2e-16 *** ## cond(DressCodeyes) 7.61e-05 *** ## cond(LocationRural) 2.63e-05 *** ## cond(LocationTown) 0.000147 *** ## cond(LocationUrban Fringe) 0.386505 ## cond(UniformsRequiredyes) 0.388948 ## cond(SecurityCamerasyes) 0.441588 ## cond(NEnrollment) 0.504120 ## cond(TrainingHours) 0.986617 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you are trying to get model-averaged coefficients from the summary output above, be sure to look for the “full average” ones and not the “conditional average” (which only includes models where the predictor was included, i.e., where the coefficient was not 0). 7.3.2 Getting Predictions from the Averaged Model ma.preds &lt;- predict(ave.model, se.fit=TRUE, type = &#39;response&#39;, backtransform = FALSE) The resulting predictions are a list with entries fit and se.fit just like we are used to. (So you could make predictions with a newdata data set and use them for prediction plots, for example. Be careful – your “new” dataset now has to include values for all candidate predictors in the full model.) Comparing with the predictions from our previous “best” model: best.school.nb2 &lt;- glmmTMB(ViolentIncidentsTotal ~ DressCode + Location, data=ssc,family=nbinom2(link=&#39;log&#39;)) pred_plot(ave.model, &#39;DressCode&#39;, ylab = &#39;N. Incidents&#39;, data = ssc, color = &#39;red&#39;) pred_plot(best.school.nb2, &#39;DressCode&#39;, ylab = &#39;N. Incidents&#39;,) So they are pretty comparable, but a little different (the differences may be bigger the more there are different models with similar IC results contributing to the average model – when one model carries almost all the weight, then the “single best” model and the model-averaging model will give almost the same results). It also makes sense that there will be a bit more uncertainty in the average model. "],
["interactions.html", "Chapter 8 Interactions 8.1 Example: Quantitative-Categorical Interaction 8.2 Categorical-Categorical Interaction Example 8.3 Quant-Quant interactions? 8.4 R code 8.5 Cautionary note", " Chapter 8 Interactions Two predictors interact when you need to know values of both in order to make an accurate prediction of the response variable value. Predictors can interact in any type of regression model (so this chapter could really be placed almost anywhere). 8.1 Example: Quantitative-Categorical Interaction gf_point(eval ~ beauty, color = ~female, shape = ~female, data = teach_beauty) %&gt;% gf_lm() Eval may go up as beauty increases, but the slope of the relationship is different for females and non-females. This is an interaction between beauty and female. 8.2 Categorical-Categorical Interaction Example gf_boxplot(eval ~ formal | female, data = teach_beauty) Perhaps Informal Dress affects eval scores, but really only for non-females – for females, formal dress doesn’t make a difference either way. The effect of formal dress is different depending on the value of female. This is an interaction between formal and female. 8.3 Quant-Quant interactions? Yes, these are possible, but very hard to visualize and conceptualize. Basically, it would mean that the slope of the line for one predictor changes gradually as the value of a second variable changes. 8.4 R code If you want to include an interaction term in a model in R, use a * rather than a + between the predictors that (may) interact. For example, based on our exploration above, we might try: beauty_mod &lt;- lm(eval ~ beauty*female + formal*female, data = teach_beauty, na.action = &#39;na.fail&#39;) summary(beauty_mod) ## ## Call: ## lm(formula = eval ~ beauty * female + formal * female, data = teach_beauty, ## na.action = &quot;na.fail&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.83418 -0.36763 0.04966 0.39789 1.07161 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.84839 0.10813 35.591 &lt;2e-16 ## beauty 0.09021 0.04744 1.902 0.0578 ## femalenot female 0.27615 0.13130 2.103 0.0360 ## formalInformal Dress 0.05751 0.11574 0.497 0.6195 ## beauty:femalenot female 0.10841 0.06452 1.680 0.0936 ## femalenot female:formalInformal Dress -0.08378 0.14276 -0.587 0.5576 ## ## (Intercept) *** ## beauty . ## femalenot female * ## formalInformal Dress ## beauty:femalenot female . ## femalenot female:formalInformal Dress ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5371 on 457 degrees of freedom ## Multiple R-squared: 0.07326, Adjusted R-squared: 0.06312 ## F-statistic: 7.226 on 5 and 457 DF, p-value: 1.587e-06 Notice the additional indicator variables in the coefficient table/model equation. Now we need to adjust the effects of the beauty predictor depending on the values of formal and female, which interact with it. We can use IC-based model selection to determine whether including these interactions in a model is important or not. require(MuMIn) dredge(beauty_mod, rank = &#39;AIC&#39;) ## Global model call: lm(formula = eval ~ beauty * female + formal * female, data = teach_beauty, ## na.action = &quot;na.fail&quot;) ## --- ## Model selection table ## (Int) bty fml frm bty:fml fml:frm df logLik AIC delta weight ## 12 3.899 0.08762 + + 5 -366.309 742.6 0.00 0.414 ## 4 3.897 0.14860 + 4 -367.868 743.7 1.12 0.236 ## 16 3.896 0.08773 + + + 6 -366.308 744.6 2.00 0.152 ## 8 3.897 0.14860 + + 5 -367.868 745.7 3.12 0.087 ## 32 3.848 0.09021 + + + + 7 -366.134 746.3 3.65 0.067 ## 24 3.833 0.14880 + + + 6 -367.560 747.1 4.50 0.044 ## 2 4.010 0.13300 3 -375.323 756.6 14.03 0.000 ## 6 4.032 0.13170 + 4 -375.249 758.5 15.88 0.000 ## 3 3.901 + 3 -378.503 763.0 20.39 0.000 ## 7 3.932 + + 4 -378.367 764.7 22.12 0.000 ## 23 3.872 + + + 5 -378.103 766.2 23.59 0.000 ## 1 3.998 2 -383.747 771.5 28.88 0.000 ## 5 4.044 + 3 -383.431 772.9 30.24 0.000 ## Models ranked by AIC(x) In the case of the particular model we fitted, the “best” model starting from this full model is actually one without interactions. If you want to explore the dataset further, you will find that actually a model where age, beauty AND female interact fits much better… 8.5 Cautionary note If you include an interaction in a regression model, you must also include the corresponding “fixed effects” – this means if you have an indicator variable/slope term for an interaction in your model, you must also have the indicator variables/slopes corresponding to the individual predictors. Our fitting functions (lm(), glm(), glmmTMB(), etc.) are smart enough to ensure this for you. So is dredge(). (It would take effort to mess this up in R.) "],
["binary-regression.html", "Chapter 9 Binary Regression 9.1 Data Source 9.2 Logistic Regression 9.3 Checking the data setup 9.4 Fitting a saturated model 9.5 Link Functions 9.6 Conditions 9.7 Model Assessment Plots 9.8 Odds Ratios 9.9 Model Selection 9.10 Prediction Plots", " Chapter 9 Binary Regression Our next goal: establish a framework for doing regression modelling when the response variable is a categorical one, with two categories. 9.1 Data Source The dataset used here is on Alaskan wood frogs, detailing some physical characteristics, habitat characteristics, and the number of developmental and other abnormalities found in the frogs. It was originally obtained from:[http://datadryad.org/resource/doi:10.5061/dryad.sq72d]. The data file can be accessed online at: [http://sldr.netlify.com/data/FrogAbnormalities.csv] ## FrogID TotalLength TailLength Stage Year RoadDistance RoadType ## 1 25 19.19701 31.8285559 Stage 43 2011 22 Gravel ## 2 29 20.99035 0.1151022 Stage 46 2012 387 Gravel ## 3 29 20.50364 0.1055217 Stage 46 2011 781 Paved ## Abnormal SkeletalAbnormality EyeAbnormality orig.id ## 1 No No No 3450 ## 2 No No No 2276 ## 3 No No No 158 9.2 Logistic Regression Recall, for linear regression we fitted a model for continuous (numeric) response variable \\(y\\) according to: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon \\] where \\(x\\)s are the \\(k\\) predictor variables, \\(\\beta\\)s are the parameters to be estimated by the model, and \\(\\epsilon \\sim N(0,\\sigma)\\) are the model residuals. When our response variable was a count variable, we modified our equation to: \\[log(\\lambda_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon_{link} \\] positing that \\(y_i \\sim Pois(\\lambda_i)\\) for Poisson regression; similarly for quasiPoisson or negative binomial regression, we just replaced that Poisson distribution with a quasiPoisson or a negative binomial distribution. What if our response variable is logical – a categorical variable with just two possible values? We will designate one of the two values a “success,” and then we want to predict the probability of success as a function of some set of predictors. What will our model equation look like in this case? \\[ logit(p_i) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ...\\beta_k x_k + \\epsilon_{link}\\] where the logit function is \\(logit(x) = log(\\frac{x}{1-x})\\). This function maps probabilities to positive and negative real numbers, effectively “spreading them out” from the range 0-1 to the full range of real numbers. How does this equation relate back to our desired response variable? Well, \\(i\\)th observation of the response variable is assumed to follow a binomial distribution with probability \\(p_i\\) (\\(y_i \\sim Binom(n_i, p_i)\\)). (\\(n_i\\) depends on the setup of the data – often n=1 for each row of the dataset, as here where each row is one frog. We can think of each frog as one binomial trial, with success/failure meaning abnormality/normality of the frog.) 9.3 Checking the data setup We would like to model the proportion frogs with abnormalities as a function of a set of covariates. The variable Abnormal has values “Yes” and “No”. In R, if we use this (factor) variable as our response, how will R determine which level (value of the variable) is a “success”? R uses the FIRST variable value as “failure” and the second as “success” – this makes sense if you imagine coding 0 for failure and 1 for success (and then sorting in numeric/alphabetical order). If you have a categorical variable with informative values, you will need to make sure that the “base” (first) level is the one you want to equate with “failure”. levels(frogs$Abnormal) ## [1] &quot;No&quot; &quot;Yes&quot; If you do need to rearrange the levels, one way to do it is to use the forcats::fct_relevel() function. Example: #ref will be the FIRST level after releveling frogs &lt;- frogs %&gt;% mutate(Abnormal = forcats::fct_relevel(Abnormal, ref=&#39;No&#39;)) frogs %&gt;% pull(Abnormal) %&gt;% levels() ## [1] &quot;No&quot; &quot;Yes&quot; 9.4 Fitting a saturated model Let’s try fitting a model for Abnormalities as a function of Stage, Year, RoadType, and RoadDistance. Why do you think these variables and not others were chosen? Perhaps it makes sense that type and distance to road are proxies for urbanization, and frogs may do better in more pristine habitats. It also seems likely that there would be differences over time. There may also be differences by Stage, if frogs with severe abnormalities have trouble even surviving to the later/older stages. frog.logr &lt;- glm(Abnormal ~ Stage + factor(Year) + RoadType + RoadDistance, data=frogs, family=binomial(link=&#39;logit&#39;)) summary(frog.logr) ## ## Call: ## glm(formula = Abnormal ~ Stage + factor(Year) + RoadType + RoadDistance, ## family = binomial(link = &quot;logit&quot;), data = frogs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.8904 -0.6398 -0.5574 -0.4788 2.2886 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.3509350 0.4198798 -0.836 0.403268 ## StageStage 43 0.1129785 0.1136966 0.994 0.320376 ## StageStage 44 -0.0987133 0.1290558 -0.765 0.444338 ## StageStage 45 -0.4616044 0.1301337 -3.547 0.000389 *** ## StageStage 46 -0.5954533 0.1451580 -4.102 4.09e-05 *** ## factor(Year)2001 -1.4231818 0.8393630 -1.696 0.089971 . ## factor(Year)2002 -1.3820162 1.1210725 -1.233 0.217664 ## factor(Year)2003 -0.9744573 0.5216890 -1.868 0.061778 . ## factor(Year)2004 -1.4345710 0.4272057 -3.358 0.000785 *** ## factor(Year)2005 -1.1185067 0.4214799 -2.654 0.007960 ** ## factor(Year)2006 -1.1232649 0.4229028 -2.656 0.007905 ** ## factor(Year)2010 -0.4985424 0.4299638 -1.159 0.246253 ## factor(Year)2011 -1.1674055 0.4163277 -2.804 0.005046 ** ## factor(Year)2012 -1.0488208 0.4157733 -2.523 0.011650 * ## RoadTypePaved -0.1689634 0.0842935 -2.004 0.045020 * ## RoadDistance 0.0005653 0.0002471 2.287 0.022176 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4787.5 on 5370 degrees of freedom ## Residual deviance: 4701.4 on 5355 degrees of freedom ## AIC: 4733.4 ## ## Number of Fisher Scoring iterations: 4 9.5 Link Functions Here, we have used the logit link function, which is the most common. However, there are other functions that translate proportions to real numbers, and are sometimes used in regression for binary data. Two common options are: Probit regression: link=‘probit’ Complementary log-log regression: link=‘cloglog’ There are not closed-form expressions for the the probit and complementary log-log functions that are easy to write down, so that is why the exact functions are not given here. As shown below, the shapes of these three functions are very similar. So it may come as no big surprise that frequently they provide similar goodness of fit to data (according to IC). If that is the case, choose logit (which makes some of the interpretation of results easier). Note: figure is from [http://data.princeton.edu/wws509/notes]. 9.6 Conditions Under what conditions is a logistic regression model appropriate? Response variable is logical – you can characterize it as the outcome of a binomial trial (or a set of independent binomial trials). Some response variables can be expressed as proportions, but can not be well modelled with binomial regression. For example, you might take one-minute recordings in the woods and measure the proportion of each minute during which bird song was audible. The data will look like proportions, but you can’t think of them as binomial trials and should not model them with binomial regression (what is a “trial” here, and what is a “success”? Make sure you can answer those questions before using binomial regression.) Linearity: logit(p) should have a linear relationship with each predictor variable. (A bit hard to check - see solutions to HW8 for an example of how it can be done.) Independence: Same as usual. Mean-variance relationship: The Pearson or Deviance residuals will decrease as a function of fitted value, and should have approximately constant variance as a function of fitted value. But a residuals vs fitted plot is of almost no use to us – the examples later on show how you can expect it to look, and if it deviates from the expected appearance, try to figure out why and what is going on; but if it looks as expected, you can say “there is no evidence in this figure of a violation of the conditions of binary regression.” NO distributional assumptions about residuals. 9.7 Model Assessment Plots gf_point(resid(frog.logr, type=&#39;pearson&#39;) ~ fitted(frog.logr)) %&gt;% gf_labs(title=&#39;frog.logr&#39;, y=&#39; Pearson\\nResiduals&#39;,x=&#39;Fitted Values&#39;) gf_point(resid(frog.logr, type=&#39;deviance&#39;) ~ fitted(frog.logr)) %&gt;% gf_labs(title=&#39;frog.logr&#39;, y=&#39; Deviance\\nResiduals&#39;,x=&#39;Fitted Values&#39;) gf_point(resid(frog.logr, type=&#39;response&#39;) ~ fitted(frog.logr)) %&gt;% gf_labs(title=&#39;frog.logr&#39;, y=&#39; Raw Response\\nResiduals&#39;,x=&#39;Fitted Values&#39;) gf_histogram(~resid(frog.logr, type=&#39;pearson&#39;), bins=15) %&gt;% gf_labs(title=&#39;frog.logr&#39;, x=&#39;Residuals&#39;, y=&#39;Count&#39;) The two “lines” in the residuals vs fitted plots correspond with the two possible values of the response variable in the data. And remember - there is not a strict distributional assumption about the residuals (in other words, they don’t have to follow, say, a normal distribution), so we don’t really have to make a histogram of them. The one here is shown just to help you remember that you don’t have to check it, and if you do, it will look “strange” (bimodal like this) yet it is nothing to worry about. 9.8 Odds Ratios The odds (or odds ratio) is \\(\\frac{p}{1-p}\\) – the ratio of success to failure. So if P(success) = 0.75, then the odds will be \\(\\frac{0.75}{0.25}\\) = 3 or “three to one” – you will usually succeed three times for every failure. Remember, the logit function was \\(logit(x) = log(\\frac{p}{1-p})\\)? In other words, the logit is the log of the odds ratio. This means that the coefficients of a binary regression model with logit link function have special interpretations in terms of odds ratios. Let’s consider a simplified version of our model (just to make it easier to write out the model equation): simple &lt;- glm(Abnormal ~ Stage, data = frogs, family=binomial(link=&#39;logit&#39;)) coef(simple) ## (Intercept) StageStage 43 StageStage 44 StageStage 45 StageStage 46 ## -1.4618893 0.1001146 -0.1094112 -0.4527238 -0.5119712 So our model equation is: \\[ logit(p_i) = -1.46 + 0.10I_{s43} - 0.11I_{s44} - 0.45I_{s45} - 0.51I_{s46}\\] According to this model, the log-odds (logit(p)) for a Stage 42 frog is -1.46, so the odds of being Abnormal for a Stage 42 frog are \\(e^{-1.46} = 0.23\\). The log-odds for a Stage 46 frog are -1.46 - 0.51 = -1.97, so the odds of it being Abnormal are \\(e^{-1.97} = 0.14\\). The change in odds going from Stage 42 to 46 is then \\(\\frac{0.14}{0.23} = 0.61\\) – the odds of a Stage 42 frog being abnormal are nearly double those of a Stage 46 frog. Notice – we didn’t actually have to compute all that to find the 0.6 value! We know that for Stage 46 \\[ log(\\frac{p}{1-p}) = -1.46 -0.51\\] so \\[ \\frac{p}{1-p} = e^{-1.46 - 0.51} = e^{-1.46}e^{-0.51}\\] And \\(e^{-1.46}\\) is the odds for Stage 42…aha! So, \\(e^{-0.51} = 0.60\\) is the multiplier on the odds ratio to go from stage 42 to 46. And in general, \\(e^{\\beta}\\) is the multiplier on the odds ratio for a one-unit change in the predictor variable for which \\(\\beta\\) is the model coefficient. 9.9 Model Selection As usual: require(MuMIn) frog.logr &lt;- update(frog.logr, na.action=&#39;na.fail&#39;) mod.sel &lt;- dredge(frog.logr, rank=&#39;AIC&#39;) head(mod.sel,5) ## Global model call: glm(formula = Abnormal ~ Stage + factor(Year) + RoadType + RoadDistance, ## family = binomial(link = &quot;logit&quot;), data = frogs, na.action = &quot;na.fail&quot;) ## --- ## Model selection table ## (Int) fct(Yer) RdD RdT Stg df logLik AIC delta weight ## 16 -0.3509 + 0.0005653 + + 16 -2350.719 4733.4 0.00 0.590 ## 12 -0.3426 + 0.0005161 + 15 -2352.740 4735.5 2.04 0.213 ## 14 -0.3749 + + + 15 -2353.232 4736.5 3.03 0.130 ## 10 -0.3653 + + 14 -2354.889 4737.8 4.34 0.067 ## 15 -1.4510 0.0004694 + + 7 -2368.278 4750.6 17.12 0.000 ## Models ranked by AIC(x) afm &lt;- model.avg(mod.sel, fit=TRUE) coef(afm) ## (Intercept) factor(Year)2001 factor(Year)2002 factor(Year)2003 ## -0.3535582013 -1.4237089763 -1.3808976121 -0.9748675931 ## factor(Year)2004 factor(Year)2005 factor(Year)2006 factor(Year)2010 ## -1.4331851168 -1.1210426509 -1.1258664109 -0.5125316516 ## factor(Year)2011 factor(Year)2012 RoadDistance RoadTypePaved ## -1.1735417137 -1.0548360421 0.0005522245 -0.1658639112 ## StageStage 43 StageStage 44 StageStage 45 StageStage 46 ## 0.1093083878 -0.1003833764 -0.4577677670 -0.5886749698 9.10 Prediction Plots Shown here are example prediction plots for Stage and RoadDistance. First, check out a summary table for the variables in the model to help determine fixed values. frogs$Year &lt;- factor(frogs$Year) summary(frogs[,c(&#39;RoadDistance&#39;, &#39;RoadType&#39;, &#39;Stage&#39;, &#39;Year&#39;)]) ## RoadDistance RoadType Stage Year ## Min. : 3.00 Gravel:3293 Stage 42: 829 2012 :1561 ## 1st Qu.: 15.00 Paved :2078 Stage 43:1417 2011 :1433 ## Median : 31.00 Stage 44: 901 2005 : 683 ## Mean : 99.38 Stage 45:1191 2006 : 647 ## 3rd Qu.: 83.00 Stage 46:1033 2004 : 605 ## Max. :781.00 2010 : 300 ## (Other): 142 require(s245) pred_plot(afm, &#39;Stage&#39;, data = frogs) pred_plot(afm, &#39;RoadDistance&#39;, data = frogs) How does this compare to the raw data? tally(~Abnormal|Stage, data=frogs, format=&#39;prop&#39;) ## Stage ## Abnormal Stage 42 Stage 43 Stage 44 Stage 45 Stage 46 ## No 0.8118215 0.7960480 0.8279689 0.8715365 0.8780252 ## Yes 0.1881785 0.2039520 0.1720311 0.1284635 0.1219748 bins &lt;- cut(frogs$RoadDistance,breaks=c(0, 25, 50, 100, 250,800)) prop(~Abnormal==&#39;Yes&#39;|bins, data=frogs) ## prop_TRUE.(0,25] prop_TRUE.(25,50] prop_TRUE.(50,100] ## 0.1516080 0.1925424 0.1379747 ## prop_TRUE.(100,250] prop_TRUE.(250,800] ## 0.1694215 0.1613876 But…remember, in the raw data, other predictors may also be influencing the patterns that you see in the data. In addition, we can look at the width of the confidence bands on the model estimates, and look at the model selection results to get an idea of whether this predictor is really important in the model or not. This is just an example to get you thinking about what prediction plots are showing you and what you can do with them! "],
["references.html", "References", " References "]
]
